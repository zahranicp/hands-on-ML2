{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c019c1",
   "metadata": {},
   "source": [
    "### Nama : Zahrani Cahya Priesa\n",
    "### NIM : 1103223074\n",
    "### Mata Kuliah : Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ca38c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CALLBACKS & TENSORBOARD\n",
      "============================================================\n",
      "TensorFlow version: 2.18.0\n",
      "\n",
      "笨 Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Import Libraries & Setup\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow & Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CALLBACKS & TENSORBOARD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"\\n笨 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92b32dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA PREPARATION - FASHION MNIST\n",
      "============================================================\n",
      "\n",
      "沒 Data Split:\n",
      "  Training: 55,000 samples\n",
      "  Validation: 5,000 samples\n",
      "  Test: 10,000 samples\n",
      "\n",
      "笨 Data preparation completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Load & Prepare Fashion MNIST\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA PREPARATION - FASHION MNIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load Fashion MNIST\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Split validation set\n",
    "X_valid, y_valid = X_train_full[:5000], y_train_full[:5000]\n",
    "X_train, y_train = X_train_full[5000:], y_train_full[5000:]\n",
    "\n",
    "# Scale pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_valid = X_valid / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(f\"\\n沒 Data Split:\")\n",
    "print(f\"  Training: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Validation: {X_valid.shape[0]:,} samples\")\n",
    "print(f\"  Test: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "print(\"\\n笨 Data preparation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f784a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODELCHECKPOINT CALLBACK\n",
      "============================================================\n",
      "\n",
      " ModelCheckpoint Purpose:\n",
      "  窶｢ Automatically save model during training\n",
      "  窶｢ Save only when model improves\n",
      "  窶｢ Prevent losing best model if training diverges\n",
      "\n",
      " ModelCheckpoint Settings:\n",
      "  Filepath: models/best_model.keras\n",
      "  Monitor: val_loss\n",
      "  Save best only: True\n",
      "\n",
      " Training with ModelCheckpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1716/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6806 - loss: 1.0064\n",
      "Epoch 1: val_loss improved from inf to 0.52610, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.6808 - loss: 1.0058 - val_accuracy: 0.8204 - val_loss: 0.5261\n",
      "Epoch 2/10\n",
      "\u001b[1m1702/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8240 - loss: 0.5048\n",
      "Epoch 2: val_loss improved from 0.52610 to 0.45600, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8241 - loss: 0.5047 - val_accuracy: 0.8422 - val_loss: 0.4560\n",
      "Epoch 3/10\n",
      "\u001b[1m1708/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8423 - loss: 0.4484\n",
      "Epoch 3: val_loss improved from 0.45600 to 0.41885, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8423 - loss: 0.4484 - val_accuracy: 0.8542 - val_loss: 0.4188\n",
      "Epoch 4/10\n",
      "\u001b[1m1703/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8540 - loss: 0.4171\n",
      "Epoch 4: val_loss improved from 0.41885 to 0.39693, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8540 - loss: 0.4171 - val_accuracy: 0.8622 - val_loss: 0.3969\n",
      "Epoch 5/10\n",
      "\u001b[1m1703/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8606 - loss: 0.3949\n",
      "Epoch 5: val_loss improved from 0.39693 to 0.37945, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8606 - loss: 0.3949 - val_accuracy: 0.8668 - val_loss: 0.3795\n",
      "Epoch 6/10\n",
      "\u001b[1m1714/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8657 - loss: 0.3777\n",
      "Epoch 6: val_loss improved from 0.37945 to 0.36754, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8657 - loss: 0.3777 - val_accuracy: 0.8704 - val_loss: 0.3675\n",
      "Epoch 7/10\n",
      "\u001b[1m1717/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8706 - loss: 0.3637\n",
      "Epoch 7: val_loss improved from 0.36754 to 0.35782, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8706 - loss: 0.3637 - val_accuracy: 0.8718 - val_loss: 0.3578\n",
      "Epoch 8/10\n",
      "\u001b[1m1709/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8749 - loss: 0.3517\n",
      "Epoch 8: val_loss improved from 0.35782 to 0.34962, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8749 - loss: 0.3517 - val_accuracy: 0.8746 - val_loss: 0.3496\n",
      "Epoch 9/10\n",
      "\u001b[1m1717/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8790 - loss: 0.3413\n",
      "Epoch 9: val_loss improved from 0.34962 to 0.34276, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8790 - loss: 0.3413 - val_accuracy: 0.8770 - val_loss: 0.3428\n",
      "Epoch 10/10\n",
      "\u001b[1m1712/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8818 - loss: 0.3318\n",
      "Epoch 10: val_loss improved from 0.34276 to 0.33596, saving model to models/best_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8818 - loss: 0.3318 - val_accuracy: 0.8822 - val_loss: 0.3360\n",
      "\n",
      "笨 Training with ModelCheckpoint completed!\n",
      "笨 Best model saved at: models/best_model.keras\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ModelCheckpoint - Save Best Model\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODELCHECKPOINT CALLBACK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n ModelCheckpoint Purpose:\")\n",
    "print(\"  窶｢ Automatically save model during training\")\n",
    "print(\"  窶｢ Save only when model improves\")\n",
    "print(\"  窶｢ Prevent losing best model if training diverges\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Build model\n",
    "model_checkpoint = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_checkpoint.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Create ModelCheckpoint callback\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    filepath=\"models/best_model.keras\",\n",
    "    save_best_only=True,  # Only save when val_loss improves\n",
    "    monitor=\"val_loss\",    # Metric to monitor\n",
    "    verbose=1              # Print message when saving\n",
    ")\n",
    "\n",
    "print(\"\\n ModelCheckpoint Settings:\")\n",
    "print(f\"  Filepath: models/best_model.keras\")\n",
    "print(f\"  Monitor: val_loss\")\n",
    "print(f\"  Save best only: True\")\n",
    "\n",
    "# Train with checkpoint\n",
    "print(\"\\n Training with ModelCheckpoint...\")\n",
    "history_cp = model_checkpoint.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[checkpoint_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n笨 Training with ModelCheckpoint completed!\")\n",
    "print(f\"笨 Best model saved at: models/best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064392cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EARLYSTOPPING CALLBACK\n",
      "============================================================\n",
      "\n",
      " EarlyStopping Purpose:\n",
      "  窶｢ Stop training when validation metric stops improving\n",
      "  窶｢ Prevent overfitting\n",
      "  窶｢ Save time (no need to train 100 epochs)\n",
      "\n",
      " EarlyStopping Settings:\n",
      "  Monitor: val_loss\n",
      "  Patience: 5 epochs\n",
      "  Restore best weights: True\n",
      "\n",
      " Training with EarlyStopping (max 50 epochs)...\n",
      "Epoch 1/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.6786 - loss: 1.0004 - val_accuracy: 0.8198 - val_loss: 0.5201\n",
      "Epoch 2/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8257 - loss: 0.4994 - val_accuracy: 0.8418 - val_loss: 0.4550\n",
      "Epoch 3/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8417 - loss: 0.4475 - val_accuracy: 0.8538 - val_loss: 0.4205\n",
      "Epoch 4/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8525 - loss: 0.4173 - val_accuracy: 0.8600 - val_loss: 0.4004\n",
      "Epoch 5/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8595 - loss: 0.3959 - val_accuracy: 0.8634 - val_loss: 0.3830\n",
      "Epoch 6/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8654 - loss: 0.3793 - val_accuracy: 0.8668 - val_loss: 0.3714\n",
      "Epoch 7/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8704 - loss: 0.3655 - val_accuracy: 0.8682 - val_loss: 0.3616\n",
      "Epoch 8/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8746 - loss: 0.3536 - val_accuracy: 0.8724 - val_loss: 0.3534\n",
      "Epoch 9/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8779 - loss: 0.3433 - val_accuracy: 0.8742 - val_loss: 0.3463\n",
      "Epoch 10/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8810 - loss: 0.3340 - val_accuracy: 0.8772 - val_loss: 0.3408\n",
      "Epoch 11/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8844 - loss: 0.3255 - val_accuracy: 0.8784 - val_loss: 0.3361\n",
      "Epoch 12/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8868 - loss: 0.3180 - val_accuracy: 0.8792 - val_loss: 0.3312\n",
      "Epoch 13/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8893 - loss: 0.3109 - val_accuracy: 0.8812 - val_loss: 0.3270\n",
      "Epoch 14/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8917 - loss: 0.3041 - val_accuracy: 0.8822 - val_loss: 0.3235\n",
      "Epoch 15/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8931 - loss: 0.2978 - val_accuracy: 0.8824 - val_loss: 0.3206\n",
      "Epoch 16/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8953 - loss: 0.2917 - val_accuracy: 0.8846 - val_loss: 0.3184\n",
      "Epoch 17/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8969 - loss: 0.2862 - val_accuracy: 0.8846 - val_loss: 0.3155\n",
      "Epoch 18/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8991 - loss: 0.2807 - val_accuracy: 0.8856 - val_loss: 0.3133\n",
      "Epoch 19/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9009 - loss: 0.2757 - val_accuracy: 0.8858 - val_loss: 0.3112\n",
      "Epoch 20/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9029 - loss: 0.2709 - val_accuracy: 0.8862 - val_loss: 0.3093\n",
      "Epoch 21/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9047 - loss: 0.2662 - val_accuracy: 0.8864 - val_loss: 0.3078\n",
      "Epoch 22/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9070 - loss: 0.2616 - val_accuracy: 0.8870 - val_loss: 0.3072\n",
      "Epoch 23/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9088 - loss: 0.2572 - val_accuracy: 0.8884 - val_loss: 0.3054\n",
      "Epoch 24/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9100 - loss: 0.2529 - val_accuracy: 0.8890 - val_loss: 0.3035\n",
      "Epoch 25/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9114 - loss: 0.2489 - val_accuracy: 0.8906 - val_loss: 0.3035\n",
      "Epoch 26/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9129 - loss: 0.2449 - val_accuracy: 0.8898 - val_loss: 0.3026\n",
      "Epoch 27/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9142 - loss: 0.2409 - val_accuracy: 0.8924 - val_loss: 0.3012\n",
      "Epoch 28/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9161 - loss: 0.2370 - val_accuracy: 0.8924 - val_loss: 0.3003\n",
      "Epoch 29/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9176 - loss: 0.2331 - val_accuracy: 0.8912 - val_loss: 0.2993\n",
      "Epoch 30/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9187 - loss: 0.2295 - val_accuracy: 0.8920 - val_loss: 0.2982\n",
      "Epoch 31/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9205 - loss: 0.2258 - val_accuracy: 0.8930 - val_loss: 0.2979\n",
      "Epoch 32/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9221 - loss: 0.2221 - val_accuracy: 0.8938 - val_loss: 0.2971\n",
      "Epoch 33/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9232 - loss: 0.2187 - val_accuracy: 0.8932 - val_loss: 0.2974\n",
      "Epoch 34/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9245 - loss: 0.2153 - val_accuracy: 0.8940 - val_loss: 0.2977\n",
      "Epoch 35/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9265 - loss: 0.2119 - val_accuracy: 0.8944 - val_loss: 0.2971\n",
      "Epoch 36/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9273 - loss: 0.2084 - val_accuracy: 0.8938 - val_loss: 0.2966\n",
      "Epoch 37/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9287 - loss: 0.2052 - val_accuracy: 0.8940 - val_loss: 0.2941\n",
      "Epoch 38/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9297 - loss: 0.2019 - val_accuracy: 0.8938 - val_loss: 0.2942\n",
      "Epoch 39/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9312 - loss: 0.1988 - val_accuracy: 0.8958 - val_loss: 0.2956\n",
      "Epoch 40/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9319 - loss: 0.1954 - val_accuracy: 0.8938 - val_loss: 0.2951\n",
      "Epoch 41/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9336 - loss: 0.1923 - val_accuracy: 0.8958 - val_loss: 0.2954\n",
      "Epoch 42/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9352 - loss: 0.1892 - val_accuracy: 0.8962 - val_loss: 0.2941\n",
      "Epoch 43/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9363 - loss: 0.1862 - val_accuracy: 0.8958 - val_loss: 0.2943\n",
      "Epoch 44/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9370 - loss: 0.1831 - val_accuracy: 0.8958 - val_loss: 0.2941\n",
      "Epoch 45/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9385 - loss: 0.1801 - val_accuracy: 0.8960 - val_loss: 0.2944\n",
      "Epoch 46/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9397 - loss: 0.1772 - val_accuracy: 0.8954 - val_loss: 0.2956\n",
      "Epoch 47/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9413 - loss: 0.1742 - val_accuracy: 0.8958 - val_loss: 0.2960\n",
      "Epoch 48/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9424 - loss: 0.1711 - val_accuracy: 0.8956 - val_loss: 0.2971\n",
      "Epoch 49/50\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9433 - loss: 0.1684 - val_accuracy: 0.8954 - val_loss: 0.2977\n",
      "Epoch 49: early stopping\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "\n",
      "笨 Training stopped at epoch 49\n",
      "笨 Best weights restored!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EarlyStopping - Prevent Overfitting\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EARLYSTOPPING CALLBACK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n EarlyStopping Purpose:\")\n",
    "print(\"  窶｢ Stop training when validation metric stops improving\")\n",
    "print(\"  窶｢ Prevent overfitting\")\n",
    "print(\"  窶｢ Save time (no need to train 100 epochs)\")\n",
    "\n",
    "# Build new model\n",
    "model_early = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_early.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Create EarlyStopping callback\n",
    "early_stopping_cb = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,           # Stop after 5 epochs without improvement\n",
    "    restore_best_weights=True,  # Restore best weights\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n EarlyStopping Settings:\")\n",
    "print(f\"  Monitor: val_loss\")\n",
    "print(f\"  Patience: 5 epochs\")\n",
    "print(f\"  Restore best weights: True\")\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"\\n Training with EarlyStopping (max 50 epochs)...\")\n",
    "history_es = model_early.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # Set high, but will stop early\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n笨 Training stopped at epoch {len(history_es.history['loss'])}\")\n",
    "print(\"笨 Best weights restored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95767c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMBINING MULTIPLE CALLBACKS\n",
      "============================================================\n",
      "\n",
      "Best Practice:\n",
      "  窶｢ Use BOTH ModelCheckpoint + EarlyStopping\n",
      "  窶｢ Checkpoint saves best model\n",
      "  窶｢ EarlyStopping prevents wasted training time\n",
      "\n",
      "Combined Callbacks:\n",
      "  1. ModelCheckpoint:\n",
      "     - Monitor: val_accuracy (max)\n",
      "     - Save best only: True\n",
      "\n",
      "  2. EarlyStopping:\n",
      "     - Monitor: val_accuracy (max)\n",
      "     - Patience: 10 epochs\n",
      "\n",
      "Training with combined callbacks...\n",
      "Epoch 1/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6122 - loss: 1.1476\n",
      "Epoch 1: val_accuracy improved from -inf to 0.82040, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6123 - loss: 1.1474 - val_accuracy: 0.8204 - val_loss: 0.5296\n",
      "Epoch 2/100\n",
      "\u001b[1m1717/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7957 - loss: 0.5878\n",
      "Epoch 2: val_accuracy improved from 0.82040 to 0.84960, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7957 - loss: 0.5878 - val_accuracy: 0.8496 - val_loss: 0.4520\n",
      "Epoch 3/100\n",
      "\u001b[1m1709/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8220 - loss: 0.5085\n",
      "Epoch 3: val_accuracy improved from 0.84960 to 0.85980, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8221 - loss: 0.5084 - val_accuracy: 0.8598 - val_loss: 0.4180\n",
      "Epoch 4/100\n",
      "\u001b[1m1714/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8336 - loss: 0.4701\n",
      "Epoch 4: val_accuracy improved from 0.85980 to 0.86480, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8336 - loss: 0.4701 - val_accuracy: 0.8648 - val_loss: 0.3963\n",
      "Epoch 5/100\n",
      "\u001b[1m1718/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8422 - loss: 0.4456\n",
      "Epoch 5: val_accuracy improved from 0.86480 to 0.86840, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8422 - loss: 0.4456 - val_accuracy: 0.8684 - val_loss: 0.3792\n",
      "Epoch 6/100\n",
      "\u001b[1m1715/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8500 - loss: 0.4232\n",
      "Epoch 6: val_accuracy improved from 0.86840 to 0.86980, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8500 - loss: 0.4232 - val_accuracy: 0.8698 - val_loss: 0.3686\n",
      "Epoch 7/100\n",
      "\u001b[1m1717/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8561 - loss: 0.4081\n",
      "Epoch 7: val_accuracy improved from 0.86980 to 0.87480, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8561 - loss: 0.4081 - val_accuracy: 0.8748 - val_loss: 0.3581\n",
      "Epoch 8/100\n",
      "\u001b[1m1709/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8587 - loss: 0.3946\n",
      "Epoch 8: val_accuracy improved from 0.87480 to 0.87820, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8587 - loss: 0.3946 - val_accuracy: 0.8782 - val_loss: 0.3503\n",
      "Epoch 9/100\n",
      "\u001b[1m1718/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8613 - loss: 0.3855\n",
      "Epoch 9: val_accuracy did not improve from 0.87820\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8613 - loss: 0.3855 - val_accuracy: 0.8760 - val_loss: 0.3439\n",
      "Epoch 10/100\n",
      "\u001b[1m1706/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8649 - loss: 0.3745\n",
      "Epoch 10: val_accuracy improved from 0.87820 to 0.87860, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8649 - loss: 0.3745 - val_accuracy: 0.8786 - val_loss: 0.3402\n",
      "Epoch 11/100\n",
      "\u001b[1m1718/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8695 - loss: 0.3640\n",
      "Epoch 11: val_accuracy did not improve from 0.87860\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8695 - loss: 0.3640 - val_accuracy: 0.8780 - val_loss: 0.3344\n",
      "Epoch 12/100\n",
      "\u001b[1m1713/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8724 - loss: 0.3549\n",
      "Epoch 12: val_accuracy improved from 0.87860 to 0.88020, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8724 - loss: 0.3549 - val_accuracy: 0.8802 - val_loss: 0.3315\n",
      "Epoch 13/100\n",
      "\u001b[1m1713/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8760 - loss: 0.3498\n",
      "Epoch 13: val_accuracy improved from 0.88020 to 0.88280, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8760 - loss: 0.3497 - val_accuracy: 0.8828 - val_loss: 0.3264\n",
      "Epoch 14/100\n",
      "\u001b[1m1715/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8772 - loss: 0.3425\n",
      "Epoch 14: val_accuracy improved from 0.88280 to 0.88340, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8772 - loss: 0.3425 - val_accuracy: 0.8834 - val_loss: 0.3236\n",
      "Epoch 15/100\n",
      "\u001b[1m1715/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8765 - loss: 0.3387\n",
      "Epoch 15: val_accuracy improved from 0.88340 to 0.88580, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8765 - loss: 0.3387 - val_accuracy: 0.8858 - val_loss: 0.3191\n",
      "Epoch 16/100\n",
      "\u001b[1m1711/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8802 - loss: 0.3337\n",
      "Epoch 16: val_accuracy improved from 0.88580 to 0.88760, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8802 - loss: 0.3336 - val_accuracy: 0.8876 - val_loss: 0.3151\n",
      "Epoch 17/100\n",
      "\u001b[1m1713/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8830 - loss: 0.3255\n",
      "Epoch 17: val_accuracy did not improve from 0.88760\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8830 - loss: 0.3254 - val_accuracy: 0.8868 - val_loss: 0.3113\n",
      "Epoch 18/100\n",
      "\u001b[1m1716/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8855 - loss: 0.3212\n",
      "Epoch 18: val_accuracy improved from 0.88760 to 0.88820, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8855 - loss: 0.3211 - val_accuracy: 0.8882 - val_loss: 0.3102\n",
      "Epoch 19/100\n",
      "\u001b[1m1705/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8855 - loss: 0.3151\n",
      "Epoch 19: val_accuracy improved from 0.88820 to 0.88960, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8855 - loss: 0.3151 - val_accuracy: 0.8896 - val_loss: 0.3071\n",
      "Epoch 20/100\n",
      "\u001b[1m1715/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8889 - loss: 0.3102\n",
      "Epoch 20: val_accuracy did not improve from 0.88960\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8889 - loss: 0.3102 - val_accuracy: 0.8890 - val_loss: 0.3050\n",
      "Epoch 21/100\n",
      "\u001b[1m1715/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8895 - loss: 0.3068\n",
      "Epoch 21: val_accuracy did not improve from 0.88960\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8895 - loss: 0.3068 - val_accuracy: 0.8884 - val_loss: 0.3057\n",
      "Epoch 22/100\n",
      "\u001b[1m1718/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8907 - loss: 0.3029\n",
      "Epoch 22: val_accuracy improved from 0.88960 to 0.89120, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8907 - loss: 0.3029 - val_accuracy: 0.8912 - val_loss: 0.3029\n",
      "Epoch 23/100\n",
      "\u001b[1m1714/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8904 - loss: 0.3016\n",
      "Epoch 23: val_accuracy did not improve from 0.89120\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8904 - loss: 0.3015 - val_accuracy: 0.8912 - val_loss: 0.2987\n",
      "Epoch 24/100\n",
      "\u001b[1m1718/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8944 - loss: 0.2937\n",
      "Epoch 24: val_accuracy did not improve from 0.89120\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8944 - loss: 0.2937 - val_accuracy: 0.8902 - val_loss: 0.2995\n",
      "Epoch 25/100\n",
      "\u001b[1m1712/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8934 - loss: 0.2917\n",
      "Epoch 25: val_accuracy did not improve from 0.89120\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8934 - loss: 0.2917 - val_accuracy: 0.8892 - val_loss: 0.2971\n",
      "Epoch 26/100\n",
      "\u001b[1m1705/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8955 - loss: 0.2887\n",
      "Epoch 26: val_accuracy did not improve from 0.89120\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8955 - loss: 0.2887 - val_accuracy: 0.8894 - val_loss: 0.2954\n",
      "Epoch 27/100\n",
      "\u001b[1m1714/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8942 - loss: 0.2864\n",
      "Epoch 27: val_accuracy improved from 0.89120 to 0.89240, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8942 - loss: 0.2864 - val_accuracy: 0.8924 - val_loss: 0.2945\n",
      "Epoch 28/100\n",
      "\u001b[1m1718/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8961 - loss: 0.2824\n",
      "Epoch 28: val_accuracy did not improve from 0.89240\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8961 - loss: 0.2824 - val_accuracy: 0.8902 - val_loss: 0.2950\n",
      "Epoch 29/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8964 - loss: 0.2823\n",
      "Epoch 29: val_accuracy did not improve from 0.89240\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8964 - loss: 0.2823 - val_accuracy: 0.8916 - val_loss: 0.2938\n",
      "Epoch 30/100\n",
      "\u001b[1m1716/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8984 - loss: 0.2776\n",
      "Epoch 30: val_accuracy did not improve from 0.89240\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8984 - loss: 0.2776 - val_accuracy: 0.8914 - val_loss: 0.2922\n",
      "Epoch 31/100\n",
      "\u001b[1m1714/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9002 - loss: 0.2717\n",
      "Epoch 31: val_accuracy improved from 0.89240 to 0.89320, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9002 - loss: 0.2717 - val_accuracy: 0.8932 - val_loss: 0.2907\n",
      "Epoch 32/100\n",
      "\u001b[1m1713/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9019 - loss: 0.2703\n",
      "Epoch 32: val_accuracy did not improve from 0.89320\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9019 - loss: 0.2703 - val_accuracy: 0.8922 - val_loss: 0.2917\n",
      "Epoch 33/100\n",
      "\u001b[1m1705/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9038 - loss: 0.2664\n",
      "Epoch 33: val_accuracy improved from 0.89320 to 0.89340, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9038 - loss: 0.2664 - val_accuracy: 0.8934 - val_loss: 0.2885\n",
      "Epoch 34/100\n",
      "\u001b[1m1705/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.2646\n",
      "Epoch 34: val_accuracy did not improve from 0.89340\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.2645 - val_accuracy: 0.8920 - val_loss: 0.2920\n",
      "Epoch 35/100\n",
      "\u001b[1m1712/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9071 - loss: 0.2608\n",
      "Epoch 35: val_accuracy improved from 0.89340 to 0.89480, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9071 - loss: 0.2608 - val_accuracy: 0.8948 - val_loss: 0.2862\n",
      "Epoch 36/100\n",
      "\u001b[1m1717/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9065 - loss: 0.2596\n",
      "Epoch 36: val_accuracy improved from 0.89480 to 0.89540, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9065 - loss: 0.2596 - val_accuracy: 0.8954 - val_loss: 0.2878\n",
      "Epoch 37/100\n",
      "\u001b[1m1716/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9076 - loss: 0.2558\n",
      "Epoch 37: val_accuracy improved from 0.89540 to 0.89600, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9076 - loss: 0.2558 - val_accuracy: 0.8960 - val_loss: 0.2852\n",
      "Epoch 38/100\n",
      "\u001b[1m1707/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9093 - loss: 0.2513\n",
      "Epoch 38: val_accuracy did not improve from 0.89600\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9093 - loss: 0.2513 - val_accuracy: 0.8948 - val_loss: 0.2849\n",
      "Epoch 39/100\n",
      "\u001b[1m1707/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9091 - loss: 0.2494\n",
      "Epoch 39: val_accuracy did not improve from 0.89600\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9091 - loss: 0.2494 - val_accuracy: 0.8942 - val_loss: 0.2871\n",
      "Epoch 40/100\n",
      "\u001b[1m1705/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9098 - loss: 0.2483\n",
      "Epoch 40: val_accuracy improved from 0.89600 to 0.89720, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9099 - loss: 0.2483 - val_accuracy: 0.8972 - val_loss: 0.2825\n",
      "Epoch 41/100\n",
      "\u001b[1m1705/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9098 - loss: 0.2472\n",
      "Epoch 41: val_accuracy improved from 0.89720 to 0.89880, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9098 - loss: 0.2472 - val_accuracy: 0.8988 - val_loss: 0.2821\n",
      "Epoch 42/100\n",
      "\u001b[1m1705/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9125 - loss: 0.2424\n",
      "Epoch 42: val_accuracy did not improve from 0.89880\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9125 - loss: 0.2424 - val_accuracy: 0.8966 - val_loss: 0.2816\n",
      "Epoch 43/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9118 - loss: 0.2397\n",
      "Epoch 43: val_accuracy did not improve from 0.89880\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9118 - loss: 0.2397 - val_accuracy: 0.8964 - val_loss: 0.2840\n",
      "Epoch 44/100\n",
      "\u001b[1m1712/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9144 - loss: 0.2368\n",
      "Epoch 44: val_accuracy did not improve from 0.89880\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9144 - loss: 0.2368 - val_accuracy: 0.8974 - val_loss: 0.2815\n",
      "Epoch 45/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9153 - loss: 0.2369\n",
      "Epoch 45: val_accuracy improved from 0.89880 to 0.90140, saving model to models/best_combined_model.keras\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9153 - loss: 0.2369 - val_accuracy: 0.9014 - val_loss: 0.2789\n",
      "Epoch 46/100\n",
      "\u001b[1m1712/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9155 - loss: 0.2331\n",
      "Epoch 46: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9155 - loss: 0.2331 - val_accuracy: 0.8968 - val_loss: 0.2840\n",
      "Epoch 47/100\n",
      "\u001b[1m1707/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9158 - loss: 0.2310\n",
      "Epoch 47: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9158 - loss: 0.2310 - val_accuracy: 0.8988 - val_loss: 0.2783\n",
      "Epoch 48/100\n",
      "\u001b[1m1708/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9164 - loss: 0.2289\n",
      "Epoch 48: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9164 - loss: 0.2289 - val_accuracy: 0.9012 - val_loss: 0.2797\n",
      "Epoch 49/100\n",
      "\u001b[1m1713/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9176 - loss: 0.2264\n",
      "Epoch 49: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9176 - loss: 0.2264 - val_accuracy: 0.8990 - val_loss: 0.2813\n",
      "Epoch 50/100\n",
      "\u001b[1m1717/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9189 - loss: 0.2251\n",
      "Epoch 50: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9189 - loss: 0.2251 - val_accuracy: 0.8986 - val_loss: 0.2782\n",
      "Epoch 51/100\n",
      "\u001b[1m1713/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9213 - loss: 0.2197\n",
      "Epoch 51: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9213 - loss: 0.2197 - val_accuracy: 0.8978 - val_loss: 0.2790\n",
      "Epoch 52/100\n",
      "\u001b[1m1717/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9187 - loss: 0.2219\n",
      "Epoch 52: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9187 - loss: 0.2219 - val_accuracy: 0.8990 - val_loss: 0.2804\n",
      "Epoch 53/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9193 - loss: 0.2186\n",
      "Epoch 53: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9193 - loss: 0.2186 - val_accuracy: 0.8982 - val_loss: 0.2804\n",
      "Epoch 54/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9203 - loss: 0.2187\n",
      "Epoch 54: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9203 - loss: 0.2187 - val_accuracy: 0.9010 - val_loss: 0.2825\n",
      "Epoch 55/100\n",
      "\u001b[1m1714/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9236 - loss: 0.2153\n",
      "Epoch 55: val_accuracy did not improve from 0.90140\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9236 - loss: 0.2153 - val_accuracy: 0.8990 - val_loss: 0.2796\n",
      "Epoch 55: early stopping\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "\n",
      "笨 Training completed!\n",
      "笨 Total epochs: 55\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Combining Multiple Callbacks\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMBINING MULTIPLE CALLBACKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nBest Practice:\")\n",
    "print(\"  窶｢ Use BOTH ModelCheckpoint + EarlyStopping\")\n",
    "print(\"  窶｢ Checkpoint saves best model\")\n",
    "print(\"  窶｢ EarlyStopping prevents wasted training time\")\n",
    "\n",
    "# Build new model\n",
    "model_combined = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dropout(0.2),  # Add dropout for regularization\n",
    "    layers.Dense(100, activation=\"relu\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_combined.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Create multiple callbacks\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    \"models/best_combined_model.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\", \n",
    "    mode=\"max\",              \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_cb = callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=10,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nCombined Callbacks:\")\n",
    "print(\"  1. ModelCheckpoint:\")\n",
    "print(\"     - Monitor: val_accuracy (max)\")\n",
    "print(\"     - Save best only: True\")\n",
    "print(\"\\n  2. EarlyStopping:\")\n",
    "print(\"     - Monitor: val_accuracy (max)\")\n",
    "print(\"     - Patience: 10 epochs\")\n",
    "\n",
    "# Train with both callbacks\n",
    "print(\"\\nTraining with combined callbacks...\")\n",
    "history_combined = model_combined.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n笨 Training completed!\")\n",
    "print(f\"笨 Total epochs: {len(history_combined.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0025ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CUSTOM CALLBACK\n",
      "============================================================\n",
      "\n",
      "Custom Callback Use Cases:\n",
      "  窶｢ Print custom messages during training\n",
      "  窶｢ Save extra information (gradients, activations)\n",
      "  窶｢ Implement custom early stopping logic\n",
      "  窶｢ Send notifications (email, Slack)\n",
      "\n",
      "Training with custom callback...\n",
      "\n",
      "Epoch 5 Summary:\n",
      "   Train Loss: 0.3953\n",
      "   Train Acc: 86.12%\n",
      "   Val Loss: 0.3879\n",
      "   Val Acc: 86.60%\n",
      "\n",
      "Epoch 10 Summary:\n",
      "   Train Loss: 0.3337\n",
      "   Train Acc: 88.05%\n",
      "   Val Loss: 0.3455\n",
      "   Val Acc: 87.82%\n",
      "\n",
      "Epoch 15 Summary:\n",
      "   Train Loss: 0.2963\n",
      "   Train Acc: 89.27%\n",
      "   Val Loss: 0.3251\n",
      "   Val Acc: 88.46%\n",
      "\n",
      "笨 Custom callback executed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Custom Callback\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CUSTOM CALLBACK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nCustom Callback Use Cases:\")\n",
    "print(\"  窶｢ Print custom messages during training\")\n",
    "print(\"  窶｢ Save extra information (gradients, activations)\")\n",
    "print(\"  窶｢ Implement custom early stopping logic\")\n",
    "print(\"  窶｢ Send notifications (email, Slack)\")\n",
    "\n",
    "# Define custom callback\n",
    "class PrintEpochResults(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Called at the end of each epoch\"\"\"\n",
    "        if (epoch + 1) % 5 == 0:  # Print every 5 epochs\n",
    "            print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "            print(f\"   Train Loss: {logs['loss']:.4f}\")\n",
    "            print(f\"   Train Acc: {logs['accuracy']*100:.2f}%\")\n",
    "            print(f\"   Val Loss: {logs['val_loss']:.4f}\")\n",
    "            print(f\"   Val Acc: {logs['val_accuracy']*100:.2f}%\")\n",
    "\n",
    "# Build model\n",
    "model_custom = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_custom.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Create custom callback\n",
    "custom_cb = PrintEpochResults()\n",
    "\n",
    "print(\"\\nTraining with custom callback...\")\n",
    "history_custom = model_custom.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=15,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[custom_cb],\n",
    "    verbose=0  # Silent, let custom callback handle printing\n",
    ")\n",
    "\n",
    "print(\"\\n笨 Custom callback executed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4773106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TENSORBOARD - TRAINING VISUALIZATION\n",
      "============================================================\n",
      "\n",
      "沒 TensorBoard Features:\n",
      "  窶｢ Real-time training metrics visualization\n",
      "  窶｢ Model graph visualization\n",
      "  窶｢ Histogram of weights/gradients\n",
      "  窶｢ Embedding visualization\n",
      "  窶｢ Profile training performance\n",
      "\n",
      "沒 Log directory: logs\\run_1767942566\n",
      "\n",
      "笞呻ｸ TensorBoard Settings:\n",
      "  Log directory: logs\\run_1767942566\n",
      "  Histogram freq: 1 (every epoch)\n",
      "  Write graph: True\n",
      "  Profile batch: 500-520\n",
      "\n",
      "泅 Training with TensorBoard logging...\n",
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.6893 - loss: 0.9911 - val_accuracy: 0.8194 - val_loss: 0.5284\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8256 - loss: 0.4978 - val_accuracy: 0.8422 - val_loss: 0.4612\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8425 - loss: 0.4443 - val_accuracy: 0.8538 - val_loss: 0.4247\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8526 - loss: 0.4140 - val_accuracy: 0.8580 - val_loss: 0.4019\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8593 - loss: 0.3929 - val_accuracy: 0.8628 - val_loss: 0.3858\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8661 - loss: 0.3763 - val_accuracy: 0.8674 - val_loss: 0.3737\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8696 - loss: 0.3627 - val_accuracy: 0.8706 - val_loss: 0.3635\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8740 - loss: 0.3510 - val_accuracy: 0.8734 - val_loss: 0.3558\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8773 - loss: 0.3407 - val_accuracy: 0.8758 - val_loss: 0.3488\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8802 - loss: 0.3314 - val_accuracy: 0.8790 - val_loss: 0.3423\n",
      "\n",
      "笨 Training with TensorBoard completed!\n",
      "\n",
      "沒 To view TensorBoard:\n",
      "  1. Open terminal/command prompt\n",
      "  2. Run: tensorboard --logdir=logs\n",
      "  3. Open browser: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TensorBoard - Visualization Tool\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TENSORBOARD - TRAINING VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n沒 TensorBoard Features:\")\n",
    "print(\"  窶｢ Real-time training metrics visualization\")\n",
    "print(\"  窶｢ Model graph visualization\")\n",
    "print(\"  窶｢ Histogram of weights/gradients\")\n",
    "print(\"  窶｢ Embedding visualization\")\n",
    "print(\"  窶｢ Profile training performance\")\n",
    "\n",
    "# Create log directory with timestamp\n",
    "import time\n",
    "log_dir = os.path.join(\"logs\", f\"run_{int(time.time())}\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n沒 Log directory: {log_dir}\")\n",
    "\n",
    "# Build model\n",
    "model_tb = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_tb.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Create TensorBoard callback\n",
    "tensorboard_cb = callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,      # Log weight histograms every epoch\n",
    "    write_graph=True,      # Visualize model graph\n",
    "    write_images=False,    # Don't save layer outputs as images\n",
    "    update_freq='epoch',   # Update logs every epoch\n",
    "    profile_batch='500,520'  # Profile batches 500-520\n",
    ")\n",
    "\n",
    "print(\"\\n笞呻ｸ TensorBoard Settings:\")\n",
    "print(f\"  Log directory: {log_dir}\")\n",
    "print(f\"  Histogram freq: 1 (every epoch)\")\n",
    "print(f\"  Write graph: True\")\n",
    "print(f\"  Profile batch: 500-520\")\n",
    "\n",
    "# Train with TensorBoard\n",
    "print(\"\\n泅 Training with TensorBoard logging...\")\n",
    "history_tb = model_tb.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[tensorboard_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n笨 Training with TensorBoard completed!\")\n",
    "print(f\"\\n沒 To view TensorBoard:\")\n",
    "print(f\"  1. Open terminal/command prompt\")\n",
    "print(f\"  2. Run: tensorboard --logdir=logs\")\n",
    "print(f\"  3. Open browser: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a713c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LEARNING RATE SCHEDULER\n",
      "============================================================\n",
      "\n",
      "沒 Why Schedule Learning Rate?\n",
      "  窶｢ Start with large LR for fast initial progress\n",
      "  窶｢ Reduce LR later for fine-tuning\n",
      "  窶｢ Helps escape local minima\n",
      "  窶｢ Improves final accuracy\n",
      "\n",
      "笞呻ｸ Learning Rate Schedule:\n",
      "  Epochs 0-9:   LR = 0.01\n",
      "  Epochs 10-19: LR = 0.005\n",
      "  Epochs 20+:   LR = 0.001\n",
      "\n",
      "泅 Training with LR scheduler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 1/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.6884 - loss: 0.9936 - val_accuracy: 0.8220 - val_loss: 0.5170 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 2/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8261 - loss: 0.4966 - val_accuracy: 0.8440 - val_loss: 0.4508 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 3/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8445 - loss: 0.4439 - val_accuracy: 0.8548 - val_loss: 0.4181 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 4/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8546 - loss: 0.4145 - val_accuracy: 0.8616 - val_loss: 0.3976 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 5/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8612 - loss: 0.3936 - val_accuracy: 0.8652 - val_loss: 0.3840 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 6/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8668 - loss: 0.3769 - val_accuracy: 0.8692 - val_loss: 0.3728 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 7/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8713 - loss: 0.3633 - val_accuracy: 0.8712 - val_loss: 0.3631 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 8/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8753 - loss: 0.3512 - val_accuracy: 0.8720 - val_loss: 0.3556 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 9/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8782 - loss: 0.3409 - val_accuracy: 0.8754 - val_loss: 0.3487 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 10/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8810 - loss: 0.3316 - val_accuracy: 0.8784 - val_loss: 0.3427 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 11/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8883 - loss: 0.3155 - val_accuracy: 0.8800 - val_loss: 0.3338 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 12/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8896 - loss: 0.3104 - val_accuracy: 0.8812 - val_loss: 0.3312 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 13/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8907 - loss: 0.3062 - val_accuracy: 0.8820 - val_loss: 0.3289 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 14/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8922 - loss: 0.3023 - val_accuracy: 0.8834 - val_loss: 0.3266 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 15/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8934 - loss: 0.2985 - val_accuracy: 0.8836 - val_loss: 0.3246 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 16/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8944 - loss: 0.2949 - val_accuracy: 0.8832 - val_loss: 0.3231 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 17/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8956 - loss: 0.2915 - val_accuracy: 0.8842 - val_loss: 0.3210 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 18/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8973 - loss: 0.2881 - val_accuracy: 0.8850 - val_loss: 0.3191 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 19/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8979 - loss: 0.2849 - val_accuracy: 0.8860 - val_loss: 0.3178 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 20/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8988 - loss: 0.2817 - val_accuracy: 0.8870 - val_loss: 0.3162 - learning_rate: 0.0050\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 21/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9032 - loss: 0.2736 - val_accuracy: 0.8888 - val_loss: 0.3134 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 22/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9042 - loss: 0.2716 - val_accuracy: 0.8898 - val_loss: 0.3131 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 23/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9048 - loss: 0.2706 - val_accuracy: 0.8898 - val_loss: 0.3129 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 24/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9050 - loss: 0.2699 - val_accuracy: 0.8896 - val_loss: 0.3126 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 25/25\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9050 - loss: 0.2691 - val_accuracy: 0.8894 - val_loss: 0.3123 - learning_rate: 0.0010\n",
      "\n",
      "笨 Training with LR scheduler completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Learning Rate Scheduler\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LEARNING RATE SCHEDULER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n沒 Why Schedule Learning Rate?\")\n",
    "print(\"  窶｢ Start with large LR for fast initial progress\")\n",
    "print(\"  窶｢ Reduce LR later for fine-tuning\")\n",
    "print(\"  窶｢ Helps escape local minima\")\n",
    "print(\"  窶｢ Improves final accuracy\")\n",
    "\n",
    "# Define learning rate schedule function\n",
    "def scheduler(epoch, lr):\n",
    "    \"\"\"Reduce LR by half every 5 epochs\"\"\"\n",
    "    if epoch < 10:\n",
    "        return 0.01\n",
    "    elif epoch < 20:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "# Create LearningRateScheduler callback\n",
    "lr_scheduler_cb = callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "# Build model\n",
    "model_lr = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_lr.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"\\n笞呻ｸ Learning Rate Schedule:\")\n",
    "print(\"  Epochs 0-9:   LR = 0.01\")\n",
    "print(\"  Epochs 10-19: LR = 0.005\")\n",
    "print(\"  Epochs 20+:   LR = 0.001\")\n",
    "\n",
    "# Train with LR scheduler\n",
    "print(\"\\n泅 Training with LR scheduler...\")\n",
    "history_lr = model_lr.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[lr_scheduler_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n笨 Training with LR scheduler completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320d0811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "REDUCELRONPLATEAU - ADAPTIVE LEARNING RATE\n",
      "============================================================\n",
      "\n",
      "沒 ReduceLROnPlateau:\n",
      "  窶｢ Automatically reduce LR when metric stops improving\n",
      "  窶｢ More adaptive than fixed schedule\n",
      "  窶｢ Monitors validation loss/accuracy\n",
      "\n",
      "笞呻ｸ ReduceLROnPlateau Settings:\n",
      "  Monitor: val_loss\n",
      "  Factor: 0.5 (reduce by half)\n",
      "  Patience: 3 epochs\n",
      "  Min LR: 0.0001\n",
      "\n",
      "泅 Training with ReduceLROnPlateau...\n",
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.6679 - loss: 1.0285 - val_accuracy: 0.8248 - val_loss: 0.5201 - learning_rate: 0.0100\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8239 - loss: 0.5022 - val_accuracy: 0.8438 - val_loss: 0.4544 - learning_rate: 0.0100\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8416 - loss: 0.4489 - val_accuracy: 0.8530 - val_loss: 0.4233 - learning_rate: 0.0100\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8525 - loss: 0.4196 - val_accuracy: 0.8610 - val_loss: 0.4003 - learning_rate: 0.0100\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8603 - loss: 0.3984 - val_accuracy: 0.8654 - val_loss: 0.3841 - learning_rate: 0.0100\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8652 - loss: 0.3821 - val_accuracy: 0.8698 - val_loss: 0.3710 - learning_rate: 0.0100\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8691 - loss: 0.3683 - val_accuracy: 0.8728 - val_loss: 0.3611 - learning_rate: 0.0100\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8729 - loss: 0.3564 - val_accuracy: 0.8752 - val_loss: 0.3542 - learning_rate: 0.0100\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8764 - loss: 0.3460 - val_accuracy: 0.8786 - val_loss: 0.3478 - learning_rate: 0.0100\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8797 - loss: 0.3367 - val_accuracy: 0.8790 - val_loss: 0.3419 - learning_rate: 0.0100\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8822 - loss: 0.3280 - val_accuracy: 0.8806 - val_loss: 0.3367 - learning_rate: 0.0100\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8858 - loss: 0.3200 - val_accuracy: 0.8824 - val_loss: 0.3322 - learning_rate: 0.0100\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8882 - loss: 0.3127 - val_accuracy: 0.8846 - val_loss: 0.3283 - learning_rate: 0.0100\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8909 - loss: 0.3058 - val_accuracy: 0.8860 - val_loss: 0.3248 - learning_rate: 0.0100\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8928 - loss: 0.2993 - val_accuracy: 0.8874 - val_loss: 0.3222 - learning_rate: 0.0100\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8954 - loss: 0.2930 - val_accuracy: 0.8880 - val_loss: 0.3190 - learning_rate: 0.0100\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8972 - loss: 0.2872 - val_accuracy: 0.8892 - val_loss: 0.3171 - learning_rate: 0.0100\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8990 - loss: 0.2817 - val_accuracy: 0.8896 - val_loss: 0.3142 - learning_rate: 0.0100\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9010 - loss: 0.2762 - val_accuracy: 0.8898 - val_loss: 0.3129 - learning_rate: 0.0100\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9028 - loss: 0.2710 - val_accuracy: 0.8892 - val_loss: 0.3105 - learning_rate: 0.0100\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9044 - loss: 0.2661 - val_accuracy: 0.8898 - val_loss: 0.3087 - learning_rate: 0.0100\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9064 - loss: 0.2613 - val_accuracy: 0.8922 - val_loss: 0.3067 - learning_rate: 0.0100\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9081 - loss: 0.2565 - val_accuracy: 0.8928 - val_loss: 0.3048 - learning_rate: 0.0100\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9097 - loss: 0.2520 - val_accuracy: 0.8922 - val_loss: 0.3026 - learning_rate: 0.0100\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9117 - loss: 0.2476 - val_accuracy: 0.8938 - val_loss: 0.3012 - learning_rate: 0.0100\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9139 - loss: 0.2433 - val_accuracy: 0.8940 - val_loss: 0.3000 - learning_rate: 0.0100\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9155 - loss: 0.2390 - val_accuracy: 0.8942 - val_loss: 0.2994 - learning_rate: 0.0100\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9171 - loss: 0.2349 - val_accuracy: 0.8944 - val_loss: 0.2989 - learning_rate: 0.0100\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9188 - loss: 0.2309 - val_accuracy: 0.8938 - val_loss: 0.2985 - learning_rate: 0.0100\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9199 - loss: 0.2271 - val_accuracy: 0.8946 - val_loss: 0.2973 - learning_rate: 0.0100\n",
      "\n",
      "笨 Training with ReduceLROnPlateau completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ReduceLROnPlateau - Adaptive LR\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REDUCELRONPLATEAU - ADAPTIVE LEARNING RATE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n沒 ReduceLROnPlateau:\")\n",
    "print(\"  窶｢ Automatically reduce LR when metric stops improving\")\n",
    "print(\"  窶｢ More adaptive than fixed schedule\")\n",
    "print(\"  窶｢ Monitors validation loss/accuracy\")\n",
    "\n",
    "# Build model\n",
    "model_plateau = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(300, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_plateau.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Create ReduceLROnPlateau callback\n",
    "reduce_lr_cb = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,           # Reduce LR by half\n",
    "    patience=3,           # After 3 epochs without improvement\n",
    "    min_lr=0.0001,        # Don't go below this\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n笞呻ｸ ReduceLROnPlateau Settings:\")\n",
    "print(\"  Monitor: val_loss\")\n",
    "print(\"  Factor: 0.5 (reduce by half)\")\n",
    "print(\"  Patience: 3 epochs\")\n",
    "print(\"  Min LR: 0.0001\")\n",
    "\n",
    "# Train with ReduceLROnPlateau\n",
    "print(\"\\n泅 Training with ReduceLROnPlateau...\")\n",
    "history_plateau = model_plateau.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[reduce_lr_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n笨 Training with ReduceLROnPlateau completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1b64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALLBACKS SUMMARY\n",
      "============================================================\n",
      "\n",
      "====================================================================================================\n",
      "             Callback                           Purpose                 Key Parameters                 Use Case\n",
      "      ModelCheckpoint     Save best model automatically        save_best_only, monitor Always use in production\n",
      "        EarlyStopping Stop training when no improvement patience, restore_best_weights  Always use to save time\n",
      "          TensorBoard   Visualize training in real-time        log_dir, histogram_freq       Debug & experiment\n",
      "LearningRateScheduler                 Fixed LR schedule             scheduler function   Known optimal schedule\n",
      "    ReduceLROnPlateau             Adaptive LR reduction       factor, patience, min_lr       Unknown optimal LR\n",
      "      Custom Callback      Custom logic during training     on_epoch_end, on_batch_end       Advanced use cases\n",
      "====================================================================================================\n",
      "\n",
      "沁ｯ BEST PRACTICES:\n",
      "------------------------------------------------------------\n",
      "1. ALWAYS use ModelCheckpoint + EarlyStopping together\n",
      "2. Use TensorBoard for debugging and experimentation\n",
      "3. Use ReduceLROnPlateau for adaptive learning rate\n",
      "4. Create custom callbacks for specific needs\n",
      "\n",
      "汳｡ PRODUCTION SETUP:\n",
      "------------------------------------------------------------\n",
      "callbacks = [\n",
      "    ModelCheckpoint('best_model.keras', save_best_only=True),\n",
      "    EarlyStopping(patience=10, restore_best_weights=True),\n",
      "    ReduceLROnPlateau(factor=0.5, patience=5),\n",
      "    TensorBoard(log_dir='logs/')\n",
      "]\n",
      "\n",
      "笨 Callbacks summary completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Summary - All Callbacks\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CALLBACKS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_data = {\n",
    "    'Callback': [\n",
    "        'ModelCheckpoint',\n",
    "        'EarlyStopping',\n",
    "        'TensorBoard',\n",
    "        'LearningRateScheduler',\n",
    "        'ReduceLROnPlateau',\n",
    "        'Custom Callback'\n",
    "    ],\n",
    "    'Purpose': [\n",
    "        'Save best model automatically',\n",
    "        'Stop training when no improvement',\n",
    "        'Visualize training in real-time',\n",
    "        'Fixed LR schedule',\n",
    "        'Adaptive LR reduction',\n",
    "        'Custom logic during training'\n",
    "    ],\n",
    "    'Key Parameters': [\n",
    "        'save_best_only, monitor',\n",
    "        'patience, restore_best_weights',\n",
    "        'log_dir, histogram_freq',\n",
    "        'scheduler function',\n",
    "        'factor, patience, min_lr',\n",
    "        'on_epoch_end, on_batch_end'\n",
    "    ],\n",
    "    'Use Case': [\n",
    "        'Always use in production',\n",
    "        'Always use to save time',\n",
    "        'Debug & experiment',\n",
    "        'Known optimal schedule',\n",
    "        'Unknown optimal LR',\n",
    "        'Advanced use cases'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_callbacks = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(df_callbacks.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n沁ｯ BEST PRACTICES:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. ALWAYS use ModelCheckpoint + EarlyStopping together\")\n",
    "print(\"2. Use TensorBoard for debugging and experimentation\")\n",
    "print(\"3. Use ReduceLROnPlateau for adaptive learning rate\")\n",
    "print(\"4. Create custom callbacks for specific needs\")\n",
    "\n",
    "print(\"\\n汳｡ PRODUCTION SETUP:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"callbacks = [\")\n",
    "print(\"    ModelCheckpoint('best_model.keras', save_best_only=True),\")\n",
    "print(\"    EarlyStopping(patience=10, restore_best_weights=True),\")\n",
    "print(\"    ReduceLROnPlateau(factor=0.5, patience=5),\")\n",
    "print(\"    TensorBoard(log_dir='logs/')\")\n",
    "print(\"]\")\n",
    "\n",
    "print(\"\\n笨 Callbacks summary completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
