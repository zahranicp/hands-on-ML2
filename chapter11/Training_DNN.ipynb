{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce66219e",
   "metadata": {},
   "source": [
    "### Nama : Zahrani Cahya Priesa\n",
    "### NIM : 1103223074\n",
    "### Kelas: TK-46-03\n",
    "### Mata Kuliah : Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aac6c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 1: Demonstrating Vanishing Gradients\n",
      "======================================================================\n",
      "\n",
      "üîç Activation Analysis (Forward Pass):\n",
      "   Layer 1: mean=0.5117, std=0.4645, saturated=69.3%\n",
      "   Layer 2: mean=0.5576, std=0.4372, saturated=50.6%\n",
      "   Layer 3: mean=0.4830, std=0.4410, saturated=51.0%\n",
      "   Layer 4: mean=0.5291, std=0.4376, saturated=49.5%\n",
      "   Layer 5: mean=0.5808, std=0.4381, saturated=53.8%\n",
      "\n",
      "‚ö†Ô∏è PROBLEM DETECTED:\n",
      "   ‚Ä¢ Sigmoid activations saturate\n",
      "   ‚Ä¢ Gradients vanish in deep layers\n",
      "   ‚Ä¢ Learning becomes ineffective\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 1: Demonstrating Vanishing Gradients Problem\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 1: Demonstrating Vanishing Gradients\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_deep_network_bad():\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28], name=\"input_flatten\"),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                     kernel_initializer=keras.initializers.RandomNormal(stddev=1.0),\n",
    "                     name=\"dense_1\"),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                     kernel_initializer=keras.initializers.RandomNormal(stddev=1.0),\n",
    "                     name=\"dense_2\"),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                     kernel_initializer=keras.initializers.RandomNormal(stddev=1.0),\n",
    "                     name=\"dense_3\"),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                     kernel_initializer=keras.initializers.RandomNormal(stddev=1.0),\n",
    "                     name=\"dense_4\"),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                     kernel_initializer=keras.initializers.RandomNormal(stddev=1.0),\n",
    "                     name=\"dense_5\"),\n",
    "        layers.Dense(10, activation='softmax', name=\"output\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Load Fashion MNIST\n",
    "(X_train, y_train), _ = keras.datasets.fashion_mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "\n",
    "model_bad = create_deep_network_bad()\n",
    "\n",
    "# =======================\n",
    "# üîë CRITICAL FIX (FINAL)\n",
    "# =======================\n",
    "activation_model = keras.Model(\n",
    "    inputs=model_bad.layers[0].input,     # ‚Üê INI KUNCI UTAMA\n",
    "    outputs=[layer.output for layer in model_bad.layers[1:6]]\n",
    ")\n",
    "\n",
    "# Analyze activations\n",
    "sample_batch = X_train[:1000]\n",
    "\n",
    "print(\"\\nüîç Activation Analysis (Forward Pass):\")\n",
    "activations = activation_model.predict(sample_batch, verbose=0)\n",
    "\n",
    "for i, act in enumerate(activations):\n",
    "    mean_act = act.mean()\n",
    "    std_act = act.std()\n",
    "    pct_saturated = ((act < 0.01) | (act > 0.99)).mean() * 100\n",
    "    print(\n",
    "        f\"   Layer {i+1}: \"\n",
    "        f\"mean={mean_act:.4f}, std={std_act:.4f}, \"\n",
    "        f\"saturated={pct_saturated:.1f}%\"\n",
    "    )\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è PROBLEM DETECTED:\")\n",
    "print(\"   ‚Ä¢ Sigmoid activations saturate\")\n",
    "print(\"   ‚Ä¢ Gradients vanish in deep layers\")\n",
    "print(\"   ‚Ä¢ Learning becomes ineffective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb5e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 4-5\n",
      "Activation Functions & Batch Normalization\n",
      "======================================================================\n",
      "\n",
      "Dataset:\n",
      "   Training: 20,000 samples\n",
      "   Validation: 2,000 samples\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 4: Activation Functions Comparison\n",
      "======================================================================\n",
      "\n",
      "Training models with different activations...\n",
      "   (10 epochs each, this will take a few minutes)\n",
      "\n",
      "======================================================================\n",
      "Training with ReLU...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Training Time: 17.88s\n",
      "   ‚úì Final Training Accuracy: 0.8680\n",
      "   ‚úì Final Validation Accuracy: 0.8490\n",
      "\n",
      "======================================================================\n",
      "Training with Leaky ReLU...\n",
      "======================================================================\n",
      "   ‚úì Training Time: 14.01s\n",
      "   ‚úì Final Training Accuracy: 0.8669\n",
      "   ‚úì Final Validation Accuracy: 0.8490\n",
      "\n",
      "======================================================================\n",
      "Training with ELU...\n",
      "======================================================================\n",
      "   ‚úì Training Time: 13.49s\n",
      "   ‚úì Final Training Accuracy: 0.8676\n",
      "   ‚úì Final Validation Accuracy: 0.8540\n",
      "\n",
      "======================================================================\n",
      "Training with SELU...\n",
      "======================================================================\n",
      "   ‚úì Training Time: 13.61s\n",
      "   ‚úì Final Training Accuracy: 0.8981\n",
      "   ‚úì Final Validation Accuracy: 0.8515\n",
      "\n",
      "======================================================================\n",
      "ACTIVATION FUNCTIONS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Activation  Train Acc  Val Acc  Time (s)\n",
      "       ELU    0.86760   0.8540 13.487924\n",
      "      SELU    0.89810   0.8515 13.614416\n",
      "      ReLU    0.86795   0.8490 17.881197\n",
      "Leaky ReLU    0.86685   0.8490 14.007352\n",
      "\n",
      "üèÜ BEST ACTIVATION: ELU (0.8540)\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 5: Batch Normalization\n",
      "======================================================================\n",
      "\n",
      "üèóÔ∏è Model 1: WITHOUT Batch Normalization\n",
      "   Architecture: Dense ‚Üí ReLU ‚Üí Dense ‚Üí ReLU ‚Üí Dense ‚Üí ReLU ‚Üí Output\n",
      "\n",
      "üèóÔ∏è Model 2: WITH Batch Normalization (before activation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Architecture: Dense ‚Üí BN ‚Üí ReLU ‚Üí Dense ‚Üí BN ‚Üí ReLU ‚Üí Dense ‚Üí BN ‚Üí ReLU ‚Üí Output\n",
      "\n",
      "üèóÔ∏è Model 3: WITH Batch Normalization (after activation)\n",
      "   Architecture: Dense ‚Üí ReLU ‚Üí BN ‚Üí Dense ‚Üí ReLU ‚Üí BN ‚Üí Dense ‚Üí ReLU ‚Üí BN ‚Üí Output\n",
      "\n",
      "======================================================================\n",
      "‚è≥ Training all models (10 epochs each)...\n",
      "======================================================================\n",
      "\n",
      "üèãÔ∏è Training: No BN...\n",
      "   ‚úì Training Time: 13.38s\n",
      "   ‚úì Final Training Accuracy: 0.8651\n",
      "   ‚úì Final Validation Accuracy: 0.8450\n",
      "\n",
      "üèãÔ∏è Training: BN Before Activation...\n",
      "   ‚úì Training Time: 16.66s\n",
      "   ‚úì Final Training Accuracy: 0.9094\n",
      "   ‚úì Final Validation Accuracy: 0.8570\n",
      "\n",
      "üèãÔ∏è Training: BN After Activation...\n",
      "   ‚úì Training Time: 16.55s\n",
      "   ‚úì Final Training Accuracy: 0.9176\n",
      "   ‚úì Final Validation Accuracy: 0.8475\n",
      "\n",
      "======================================================================\n",
      "üìä BATCH NORMALIZATION COMPARISON\n",
      "======================================================================\n",
      "\n",
      "               Model  Train Acc  Val Acc  Time (s)\n",
      "BN Before Activation    0.90945   0.8570 16.662968\n",
      " BN After Activation    0.91755   0.8475 16.552445\n",
      "               No BN    0.86510   0.8450 13.375244\n",
      "\n",
      "======================================================================\n",
      "üìà CONVERGENCE SPEED COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Validation Accuracy by Epoch:\n",
      "----------------------------------------------------------------------\n",
      "Epoch    No BN           BN Before       BN After       \n",
      "----------------------------------------------------------------------\n",
      "1        0.7885          0.8070          0.7965         \n",
      "2        0.8060          0.8385          0.8320         \n",
      "3        0.8170          0.8505          0.8310         \n",
      "4        0.8240          0.8525          0.8315         \n",
      "5        0.8285          0.8555          0.8395         \n",
      "6        0.8300          0.8540          0.8415         \n",
      "7        0.8335          0.8575          0.8465         \n",
      "8        0.8380          0.8615          0.8475         \n",
      "9        0.8395          0.8630          0.8515         \n",
      "10       0.8450          0.8570          0.8475         \n",
      "\n",
      "üí° IMPROVEMENT WITH BN: +0.0120 (+1.20%)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 4-5 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 4-5 : Activation Functions & Batch Normalization\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 4-5\")\n",
    "print(\"Activation Functions & Batch Normalization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Prepare subsets for faster experimentation\n",
    "X_train_subset = X_train[:20000]\n",
    "y_train_subset = y_train[:20000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"   Training: {X_train_subset.shape[0]:,} samples\")\n",
    "print(f\"   Validation: {X_val.shape[0]:,} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 4: Comparing Activation Functions\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 4: Activation Functions Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_model_with_activation(activation, use_lecun_init=False):\n",
    "    \"\"\"Create model with specified activation function\"\"\"\n",
    "    if use_lecun_init:\n",
    "        init = 'lecun_normal'\n",
    "    else:\n",
    "        if activation in ['relu', 'elu']:\n",
    "            init = 'he_normal'\n",
    "        else:\n",
    "            init = 'glorot_uniform'\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different activations\n",
    "activations_to_test = {\n",
    "    'ReLU': 'relu',\n",
    "    'Leaky ReLU': layers.LeakyReLU(alpha=0.01),\n",
    "    'ELU': 'elu',\n",
    "    'SELU': 'selu'\n",
    "}\n",
    "\n",
    "print(\"\\nTraining models with different activations...\")\n",
    "print(\"   (10 epochs each, this will take a few minutes)\")\n",
    "\n",
    "activation_results = {}\n",
    "\n",
    "for name, activation in activations_to_test.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training with {name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # For SELU, need special handling\n",
    "    if name == 'SELU':\n",
    "        # Standardize inputs for SELU\n",
    "        X_train_std = (X_train_subset - X_train_subset.mean()) / X_train_subset.std()\n",
    "        X_val_std = (X_val - X_train_subset.mean()) / X_train_subset.std()\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            layers.Flatten(input_shape=[28, 28]),\n",
    "            layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        train_data = (X_train_std, y_train_subset)\n",
    "        val_data = (X_val_std, y_val)\n",
    "    else:\n",
    "        if isinstance(activation, str):\n",
    "            model = create_model_with_activation(activation)\n",
    "        else:  # LeakyReLU case\n",
    "            model = keras.Sequential([\n",
    "                layers.Flatten(input_shape=[28, 28]),\n",
    "                layers.Dense(100, kernel_initializer='he_normal'),\n",
    "                activation,\n",
    "                layers.Dense(100, kernel_initializer='he_normal'),\n",
    "                activation,\n",
    "                layers.Dense(100, kernel_initializer='he_normal'),\n",
    "                activation,\n",
    "                layers.Dense(10, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        train_data = (X_train_subset, y_train_subset)\n",
    "        val_data = (X_val, y_val)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_data[0], train_data[1],\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=val_data,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    activation_results[name] = {\n",
    "        'history': history.history,\n",
    "        'time': training_time,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Training Time: {training_time:.2f}s\")\n",
    "    print(f\"   ‚úì Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIVATION FUNCTIONS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = []\n",
    "for name, results in activation_results.items():\n",
    "    summary_data.append({\n",
    "        'Activation': name,\n",
    "        'Train Acc': results['final_train_acc'],\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Time (s)': results['time']\n",
    "    })\n",
    "\n",
    "df_activations = pd.DataFrame(summary_data)\n",
    "df_activations = df_activations.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_activations.to_string(index=False))\n",
    "\n",
    "# Find best activation\n",
    "best_activation = df_activations.iloc[0]['Activation']\n",
    "best_val_acc = df_activations.iloc[0]['Val Acc']\n",
    "print(f\"\\nüèÜ BEST ACTIVATION: {best_activation} ({best_val_acc:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 5: Batch Normalization Impact\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 5: Batch Normalization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Model WITHOUT Batch Normalization\n",
    "print(\"\\nüèóÔ∏è Model 1: WITHOUT Batch Normalization\")\n",
    "model_no_bn = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_no_bn.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   Architecture: Dense ‚Üí ReLU ‚Üí Dense ‚Üí ReLU ‚Üí Dense ‚Üí ReLU ‚Üí Output\")\n",
    "\n",
    "# Model WITH Batch Normalization (before activation)\n",
    "print(\"\\nüèóÔ∏è Model 2: WITH Batch Normalization (before activation)\")\n",
    "model_with_bn = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_with_bn.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   Architecture: Dense ‚Üí BN ‚Üí ReLU ‚Üí Dense ‚Üí BN ‚Üí ReLU ‚Üí Dense ‚Üí BN ‚Üí ReLU ‚Üí Output\")\n",
    "\n",
    "# Model WITH Batch Normalization (after activation)\n",
    "print(\"\\nüèóÔ∏è Model 3: WITH Batch Normalization (after activation)\")\n",
    "model_bn_after = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_bn_after.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   Architecture: Dense ‚Üí ReLU ‚Üí BN ‚Üí Dense ‚Üí ReLU ‚Üí BN ‚Üí Dense ‚Üí ReLU ‚Üí BN ‚Üí Output\")\n",
    "\n",
    "# Train all models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚è≥ Training all models (10 epochs each)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bn_results = {}\n",
    "\n",
    "for model_name, model in [\n",
    "    ('No BN', model_no_bn),\n",
    "    ('BN Before Activation', model_with_bn),\n",
    "    ('BN After Activation', model_bn_after)\n",
    "]:\n",
    "    print(f\"\\nüèãÔ∏è Training: {model_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_subset, y_train_subset,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    bn_results[model_name] = {\n",
    "        'history': history.history,\n",
    "        'time': training_time,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Training Time: {training_time:.2f}s\")\n",
    "    print(f\"   ‚úì Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä BATCH NORMALIZATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bn_summary = []\n",
    "for name, results in bn_results.items():\n",
    "    bn_summary.append({\n",
    "        'Model': name,\n",
    "        'Train Acc': results['final_train_acc'],\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Time (s)': results['time']\n",
    "    })\n",
    "\n",
    "df_bn = pd.DataFrame(bn_summary)\n",
    "df_bn = df_bn.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_bn.to_string(index=False))\n",
    "\n",
    "# Epoch-by-epoch comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà CONVERGENCE SPEED COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nValidation Accuracy by Epoch:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Epoch':<8} {'No BN':<15} {'BN Before':<15} {'BN After':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(10):\n",
    "    no_bn_acc = bn_results['No BN']['history']['val_accuracy'][epoch]\n",
    "    bn_before_acc = bn_results['BN Before Activation']['history']['val_accuracy'][epoch]\n",
    "    bn_after_acc = bn_results['BN After Activation']['history']['val_accuracy'][epoch]\n",
    "    \n",
    "    print(f\"{epoch+1:<8} {no_bn_acc:<15.4f} {bn_before_acc:<15.4f} {bn_after_acc:<15.4f}\")\n",
    "\n",
    "# Find improvement\n",
    "no_bn_final = bn_results['No BN']['final_val_acc']\n",
    "best_bn_final = max(bn_results['BN Before Activation']['final_val_acc'], \n",
    "                    bn_results['BN After Activation']['final_val_acc'])\n",
    "improvement = best_bn_final - no_bn_final\n",
    "\n",
    "print(f\"\\nüí° IMPROVEMENT WITH BN: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 4-5 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea487cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 6-7\n",
      "Transfer Learning & Gradient Clipping\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 6: Gradient Clipping\n",
      "======================================================================\n",
      "\n",
      " Dataset: 20,000 training samples\n",
      "\n",
      " Testing Gradient Clipping Strategies:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Training with: No Clipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Time: 26.41s\n",
      "   ‚úì Final Val Acc: 0.8530\n",
      "\n",
      "Training with: Clip by Value (1.0)\n",
      "   ‚úì Time: 23.84s\n",
      "   ‚úì Final Val Acc: 0.8490\n",
      "\n",
      "Training with: Clip by Norm (1.0)\n",
      "   ‚úì Time: 23.69s\n",
      "   ‚úì Final Val Acc: 0.8540\n",
      "\n",
      "======================================================================\n",
      " GRADIENT CLIPPING COMPARISON\n",
      "======================================================================\n",
      "\n",
      "           Strategy  Val Acc  Time (s)\n",
      " Clip by Norm (1.0)    0.854 23.692467\n",
      "        No Clipping    0.853 26.411210\n",
      "Clip by Value (1.0)    0.849 23.843929\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENT 7: Transfer Learning\n",
      "======================================================================\n",
      "\n",
      " SCENARIO:\n",
      "   Task A: Train on Fashion MNIST classes 0-4 (5 classes)\n",
      "   Task B: Fine-tune for classes 5-9 (5 different classes)\n",
      "   Simulate: Pre-training ‚Üí Transfer ‚Üí Fine-tuning\n",
      "\n",
      " Task A Dataset (classes 0-4):\n",
      "   Training: 15,000 samples\n",
      "   Test: 1,000 samples\n",
      "\n",
      " Task B Dataset (classes 5-9, LIMITED DATA):\n",
      "   Training: 5,000 samples (only 1/3 of Task A!)\n",
      "   Test: 1,000 samples\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 1: Pre-training on Task A\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèãÔ∏è Pre-training on Task A (15 epochs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Task A Validation Accuracy: 0.8540\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 2: Transfer Learning to Task B\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèóÔ∏è BASELINE: Training from scratch on Task B (limited data)\n",
      "   ‚úì From Scratch Val Acc: 0.9370\n",
      "\n",
      " TRANSFER: Reuse pre-trained layers, freeze them\n",
      "   Frozen layers: hidden1, hidden2, hidden3\n",
      "   Trainable: output_B only\n",
      "   ‚úì Transfer (Frozen) Val Acc: 0.8120\n",
      "\n",
      " FINE-TUNE: Reuse pre-trained layers, train all\n",
      "   All layers trainable\n",
      "   Lower learning rate (0.001) for fine-tuning\n",
      "   ‚úì Transfer (Fine-tune) Val Acc: 0.8990\n",
      "\n",
      "======================================================================\n",
      " TRANSFER LEARNING COMPARISON\n",
      "======================================================================\n",
      "\n",
      "               Strategy  Val Acc          Description\n",
      "From Scratch (baseline)    0.937      No pre-training\n",
      "   Transfer (Fine-tune)    0.899 Fine-tune all layers\n",
      "      Transfer (Frozen)    0.812  Freeze lower layers\n",
      "\n",
      " TRANSFER LEARNING GAINS:\n",
      "   Frozen layers: -0.1250 (-12.50%)\n",
      "   Fine-tuning:   -0.0380 (-3.80%)\n",
      "\n",
      "======================================================================\n",
      " CONVERGENCE COMPARISON (First 10 Epochs)\n",
      "======================================================================\n",
      "Epoch    Scratch         Frozen          Fine-tune      \n",
      "----------------------------------------------------------------------\n",
      "1        0.8460          0.5880          0.6580         \n",
      "2        0.8860          0.6900          0.7160         \n",
      "3        0.9060          0.7190          0.7500         \n",
      "4        0.9150          0.7290          0.7710         \n",
      "5        0.9280          0.7460          0.7880         \n",
      "6        0.9310          0.7620          0.8050         \n",
      "7        0.9340          0.7740          0.8210         \n",
      "8        0.9350          0.7780          0.8370         \n",
      "9        0.9370          0.7860          0.8470         \n",
      "10       0.9350          0.7960          0.8610         \n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 6-7 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 6-7\n",
    "# Transfer Learning & Gradient Clipping\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 6-7\")\n",
    "print(\"Transfer Learning & Gradient Clipping\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 6: Gradient Clipping\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 6: Gradient Clipping\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "X_train_subset = X_train[:20000]\n",
    "y_train_subset = y_train[:20000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "print(f\"\\n Dataset: {X_train_subset.shape[0]:,} training samples\")\n",
    "\n",
    "# Create a model that might have exploding gradients\n",
    "def create_deep_model():\n",
    "    \"\"\"Very deep model with potential gradient issues\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different gradient clipping strategies\n",
    "print(\"\\n Testing Gradient Clipping Strategies:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "clipping_configs = {\n",
    "    'No Clipping': None,\n",
    "    'Clip by Value (1.0)': keras.optimizers.SGD(learning_rate=0.01, clipvalue=1.0),\n",
    "    'Clip by Norm (1.0)': keras.optimizers.SGD(learning_rate=0.01, clipnorm=1.0),\n",
    "}\n",
    "\n",
    "clipping_results = {}\n",
    "\n",
    "for config_name, optimizer in clipping_configs.items():\n",
    "    print(f\"\\nTraining with: {config_name}\")\n",
    "    \n",
    "    model = create_deep_model()\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_subset, y_train_subset,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    clipping_results[config_name] = {\n",
    "        'history': history.history,\n",
    "        'time': training_time,\n",
    "        'final_val_acc': history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Time: {training_time:.2f}s\")\n",
    "    print(f\"   ‚úì Final Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" GRADIENT CLIPPING COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "clip_summary = []\n",
    "for name, results in clipping_results.items():\n",
    "    clip_summary.append({\n",
    "        'Strategy': name,\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Time (s)': results['time']\n",
    "    })\n",
    "\n",
    "df_clip = pd.DataFrame(clip_summary)\n",
    "df_clip = df_clip.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_clip.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 7: Transfer Learning Simulation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EXPERIMENT 7: Transfer Learning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n SCENARIO:\")\n",
    "print(\"   Task A: Train on Fashion MNIST classes 0-4 (5 classes)\")\n",
    "print(\"   Task B: Fine-tune for classes 5-9 (5 different classes)\")\n",
    "print(\"   Simulate: Pre-training ‚Üí Transfer ‚Üí Fine-tuning\")\n",
    "\n",
    "# Prepare datasets for Task A (classes 0-4)\n",
    "mask_train_A = y_train < 5\n",
    "X_train_A = X_train[mask_train_A][:15000]\n",
    "y_train_A = y_train[mask_train_A][:15000]\n",
    "\n",
    "mask_test_A = y_test < 5\n",
    "X_test_A = X_test[mask_test_A][:1000]\n",
    "y_test_A = y_test[mask_test_A][:1000]\n",
    "\n",
    "print(f\"\\n Task A Dataset (classes 0-4):\")\n",
    "print(f\"   Training: {X_train_A.shape[0]:,} samples\")\n",
    "print(f\"   Test: {X_test_A.shape[0]:,} samples\")\n",
    "\n",
    "# Prepare datasets for Task B (classes 5-9)\n",
    "mask_train_B = y_train >= 5\n",
    "X_train_B = X_train[mask_train_B][:5000]  # Less data!\n",
    "y_train_B = y_train[mask_train_B][:5000] - 5  # Remap to 0-4\n",
    "\n",
    "mask_test_B = y_test >= 5\n",
    "X_test_B = X_test[mask_test_B][:1000]\n",
    "y_test_B = y_test[mask_test_B][:1000] - 5  # Remap to 0-4\n",
    "\n",
    "print(f\"\\n Task B Dataset (classes 5-9, LIMITED DATA):\")\n",
    "print(f\"   Training: {X_train_B.shape[0]:,} samples (only 1/3 of Task A!)\")\n",
    "print(f\"   Test: {X_test_B.shape[0]:,} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Pre-train on Task A\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 1: Pre-training on Task A\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "model_pretrain = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal', name='hidden1'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal', name='hidden2'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal', name='hidden3'),\n",
    "    layers.Dense(5, activation='softmax', name='output_A')  # 5 classes\n",
    "])\n",
    "\n",
    "model_pretrain.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nüèãÔ∏è Pre-training on Task A (15 epochs)...\")\n",
    "history_pretrain = model_pretrain.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_A, y_test_A),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "pretrain_acc = history_pretrain.history['val_accuracy'][-1]\n",
    "print(f\"   ‚úì Task A Validation Accuracy: {pretrain_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Transfer Learning - Reuse Lower Layers\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 2: Transfer Learning to Task B\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Strategy 1: Train from scratch (baseline)\n",
    "print(\"\\nüèóÔ∏è BASELINE: Training from scratch on Task B (limited data)\")\n",
    "model_scratch = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(5, activation='softmax')  # 5 classes\n",
    "])\n",
    "\n",
    "model_scratch.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_scratch = model_scratch.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_B, y_test_B),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scratch_acc = history_scratch.history['val_accuracy'][-1]\n",
    "print(f\"   ‚úì From Scratch Val Acc: {scratch_acc:.4f}\")\n",
    "\n",
    "# Strategy 2: Transfer Learning - Freeze lower layers\n",
    "print(\"\\n TRANSFER: Reuse pre-trained layers, freeze them\")\n",
    "\n",
    "# Create new model reusing lower layers\n",
    "model_transfer_frozen = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    model_pretrain.get_layer('hidden1'),\n",
    "    model_pretrain.get_layer('hidden2'),\n",
    "    model_pretrain.get_layer('hidden3'),\n",
    "    layers.Dense(5, activation='softmax', name='output_B')  # New output layer\n",
    "])\n",
    "\n",
    "# Freeze the transferred layers\n",
    "for layer in model_transfer_frozen.layers[1:4]:  # hidden1, hidden2, hidden3\n",
    "    layer.trainable = False\n",
    "\n",
    "model_transfer_frozen.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   Frozen layers: hidden1, hidden2, hidden3\")\n",
    "print(\"   Trainable: output_B only\")\n",
    "\n",
    "history_frozen = model_transfer_frozen.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_B, y_test_B),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "frozen_acc = history_frozen.history['val_accuracy'][-1]\n",
    "print(f\"   ‚úì Transfer (Frozen) Val Acc: {frozen_acc:.4f}\")\n",
    "\n",
    "# Strategy 3: Transfer Learning - Fine-tune all layers\n",
    "print(\"\\n FINE-TUNE: Reuse pre-trained layers, train all\")\n",
    "\n",
    "model_transfer_finetune = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    model_pretrain.get_layer('hidden1'),\n",
    "    model_pretrain.get_layer('hidden2'),\n",
    "    model_pretrain.get_layer('hidden3'),\n",
    "    layers.Dense(5, activation='softmax', name='output_B2')\n",
    "])\n",
    "\n",
    "# All layers trainable\n",
    "for layer in model_transfer_finetune.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_transfer_finetune.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.001),  # Lower LR for fine-tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   All layers trainable\")\n",
    "print(\"   Lower learning rate (0.001) for fine-tuning\")\n",
    "\n",
    "history_finetune = model_transfer_finetune.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_B, y_test_B),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "finetune_acc = history_finetune.history['val_accuracy'][-1]\n",
    "print(f\"   ‚úì Transfer (Fine-tune) Val Acc: {finetune_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Compare Transfer Learning Strategies\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" TRANSFER LEARNING COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "transfer_summary = pd.DataFrame([\n",
    "    {'Strategy': 'From Scratch (baseline)', 'Val Acc': scratch_acc, 'Description': 'No pre-training'},\n",
    "    {'Strategy': 'Transfer (Frozen)', 'Val Acc': frozen_acc, 'Description': 'Freeze lower layers'},\n",
    "    {'Strategy': 'Transfer (Fine-tune)', 'Val Acc': finetune_acc, 'Description': 'Fine-tune all layers'}\n",
    "])\n",
    "\n",
    "transfer_summary = transfer_summary.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + transfer_summary.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "improvement_frozen = frozen_acc - scratch_acc\n",
    "improvement_finetune = finetune_acc - scratch_acc\n",
    "\n",
    "print(f\"\\n TRANSFER LEARNING GAINS:\")\n",
    "print(f\"   Frozen layers: {improvement_frozen:+.4f} ({improvement_frozen*100:+.2f}%)\")\n",
    "print(f\"   Fine-tuning:   {improvement_finetune:+.4f} ({improvement_finetune*100:+.2f}%)\")\n",
    "\n",
    "# Convergence comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" CONVERGENCE COMPARISON (First 10 Epochs)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Epoch':<8} {'Scratch':<15} {'Frozen':<15} {'Fine-tune':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(10):\n",
    "    scratch = history_scratch.history['val_accuracy'][epoch]\n",
    "    frozen = history_frozen.history['val_accuracy'][epoch]\n",
    "    finetune = history_finetune.history['val_accuracy'][epoch]\n",
    "    print(f\"{epoch+1:<8} {scratch:<15.4f} {frozen:<15.4f} {finetune:<15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 6-7 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b009048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 8-9\n",
      "Advanced Optimizers & Learning Rate Scheduling\n",
      "======================================================================\n",
      "\n",
      " Dataset: 20,000 training samples\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENT 8: Optimizer Comparison\n",
      "======================================================================\n",
      "\n",
      " Training with different optimizers (15 epochs each)...\n",
      "   This will take several minutes...\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with SGD (vanilla)...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Time: 24.30s\n",
      "   ‚úì Final Train Acc: 0.8828\n",
      "   ‚úì Final Val Acc: 0.8575\n",
      "   ‚úì Best Val Acc: 0.8575\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with SGD + Momentum...\n",
      "======================================================================\n",
      "   ‚úì Time: 24.60s\n",
      "   ‚úì Final Train Acc: 0.9118\n",
      "   ‚úì Final Val Acc: 0.8515\n",
      "   ‚úì Best Val Acc: 0.8670\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with SGD + Nesterov...\n",
      "======================================================================\n",
      "   ‚úì Time: 26.09s\n",
      "   ‚úì Final Train Acc: 0.9166\n",
      "   ‚úì Final Val Acc: 0.8535\n",
      "   ‚úì Best Val Acc: 0.8650\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Adagrad...\n",
      "======================================================================\n",
      "   ‚úì Time: 24.80s\n",
      "   ‚úì Final Train Acc: 0.8996\n",
      "   ‚úì Final Val Acc: 0.8675\n",
      "   ‚úì Best Val Acc: 0.8680\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with RMSprop...\n",
      "======================================================================\n",
      "   ‚úì Time: 25.82s\n",
      "   ‚úì Final Train Acc: 0.9090\n",
      "   ‚úì Final Val Acc: 0.8620\n",
      "   ‚úì Best Val Acc: 0.8620\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Adam...\n",
      "======================================================================\n",
      "   ‚úì Time: 26.88s\n",
      "   ‚úì Final Train Acc: 0.9211\n",
      "   ‚úì Final Val Acc: 0.8600\n",
      "   ‚úì Best Val Acc: 0.8700\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Nadam...\n",
      "======================================================================\n",
      "   ‚úì Time: 29.70s\n",
      "   ‚úì Final Train Acc: 0.9277\n",
      "   ‚úì Final Val Acc: 0.8610\n",
      "   ‚úì Best Val Acc: 0.8640\n",
      "\n",
      "======================================================================\n",
      " OPTIMIZER COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "     Optimizer  Final Val Acc  Best Val Acc  Time (s)\n",
      "          Adam         0.8600        0.8700 26.884117\n",
      "       Adagrad         0.8675        0.8680 24.801463\n",
      "SGD + Momentum         0.8515        0.8670 24.602160\n",
      "SGD + Nesterov         0.8535        0.8650 26.090216\n",
      "         Nadam         0.8610        0.8640 29.695302\n",
      "       RMSprop         0.8620        0.8620 25.819367\n",
      " SGD (vanilla)         0.8575        0.8575 24.301309\n",
      "\n",
      " BEST OPTIMIZER: Adam (0.8700)\n",
      "\n",
      "======================================================================\n",
      " CONVERGENCE SPEED (Validation Accuracy by Epoch)\n",
      "======================================================================\n",
      "Epoch   SGD        Momentum   RMSprop    Adam       Nadam     \n",
      "----------------------------------------------------------------------\n",
      "1       0.7800     0.7815     0.8215     0.8130     0.8155    \n",
      "2       0.8175     0.8105     0.8430     0.8445     0.8345    \n",
      "3       0.8300     0.8290     0.8465     0.8490     0.8520    \n",
      "4       0.8320     0.8465     0.8470     0.8585     0.8450    \n",
      "5       0.8410     0.8575     0.8600     0.8700     0.8515    \n",
      "6       0.8465     0.8650     0.8560     0.8605     0.8580    \n",
      "7       0.8470     0.8605     0.8550     0.8635     0.8620    \n",
      "8       0.8515     0.8670     0.8520     0.8570     0.8620    \n",
      "9       0.8505     0.8565     0.8505     0.8615     0.8640    \n",
      "10      0.8535     0.8525     0.8525     0.8530     0.8575    \n",
      "11      0.8540     0.8585     0.8520     0.8670     0.8630    \n",
      "12      0.8550     0.8625     0.8490     0.8580     0.8635    \n",
      "13      0.8550     0.8585     0.8495     0.8580     0.8600    \n",
      "14      0.8575     0.8640     0.8505     0.8535     0.8620    \n",
      "15      0.8575     0.8515     0.8620     0.8600     0.8610    \n",
      "\n",
      "======================================================================\n",
      " EXPERIMENT 9: Learning Rate Scheduling\n",
      "======================================================================\n",
      "\n",
      " Extended Dataset: 30,000 training samples\n",
      "   Training for 30 epochs to see scheduling effects\n",
      "\n",
      " BASELINE: Constant Learning Rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Best Val Acc: 0.8720\n",
      "\n",
      " EXPONENTIAL DECAY SCHEDULING\n",
      "   ‚úì Best Val Acc: 0.8815\n",
      "\n",
      " PIECEWISE CONSTANT SCHEDULING\n",
      "   ‚úì Best Val Acc: 0.8815\n",
      "\n",
      " REDUCE LR ON PLATEAU\n",
      "   ‚úì Best Val Acc: 0.8930\n",
      "\n",
      "======================================================================\n",
      " LEARNING RATE SCHEDULING COMPARISON\n",
      "======================================================================\n",
      "\n",
      "          Strategy  Final Val Acc  Best Val Acc\n",
      " ReduceLROnPlateau         0.8930        0.8930\n",
      " Exponential Decay         0.8740        0.8815\n",
      "Piecewise Constant         0.8795        0.8815\n",
      "       Constant LR         0.8580        0.8720\n",
      "\n",
      "======================================================================\n",
      " VALIDATION ACCURACY OVER TIME (First 20 Epochs)\n",
      "======================================================================\n",
      "Epoch   Constant     Exponential  Piecewise    Plateau     \n",
      "----------------------------------------------------------------------\n",
      "1       0.8470       0.8505       0.8490       0.8360      \n",
      "2       0.8495       0.8565       0.8510       0.8600      \n",
      "3       0.8530       0.8575       0.8605       0.8565      \n",
      "4       0.8620       0.8595       0.8735       0.8605      \n",
      "5       0.8585       0.8695       0.8715       0.8580      \n",
      "6       0.8585       0.8740       0.8800       0.8615      \n",
      "7       0.8560       0.8745       0.8695       0.8570      \n",
      "8       0.8705       0.8720       0.8700       0.8665      \n",
      "9       0.8670       0.8700       0.8695       0.8700      \n",
      "10      0.8620       0.8650       0.8705       0.8720      \n",
      "11      0.8685       0.8705       0.8740       0.8735      \n",
      "12      0.8710       0.8710       0.8740       0.8720      \n",
      "13      0.8710       0.8755       0.8735       0.8700      \n",
      "14      0.8570       0.8770       0.8760       0.8815      \n",
      "15      0.8660       0.8785       0.8755       0.8790      \n",
      "16      0.8675       0.8790       0.8715       0.8825      \n",
      "17      0.8595       0.8815       0.8685       0.8800      \n",
      "18      0.8580       0.8810       0.8685       0.8850      \n",
      "19      0.8590       0.8795       0.8685       0.8850      \n",
      "20      0.8600       0.8755       0.8680       0.8845      \n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 8-9 COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "   ‚Ä¢ Modern optimizers (Adam, RMSprop) converge MUCH faster than SGD\n",
      "   ‚Ä¢ Momentum significantly improves SGD performance\n",
      "   ‚Ä¢ Learning rate scheduling helps achieve better final accuracy\n",
      "   ‚Ä¢ ReduceLROnPlateau is adaptive and often best for real-world tasks\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 8-9\n",
    "# Advanced Optimizers & Learning Rate Scheduling\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 8-9\")\n",
    "print(\"Advanced Optimizers & Learning Rate Scheduling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "X_train_subset = X_train[:20000]\n",
    "y_train_subset = y_train[:20000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "print(f\"\\n Dataset: {X_train_subset.shape[0]:,} training samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 8: Comparing Optimizers\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EXPERIMENT 8: Optimizer Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_standard_model():\n",
    "    \"\"\"Standard model for optimizer comparison\"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# Test different optimizers\n",
    "optimizers_config = {\n",
    "    'SGD (vanilla)': keras.optimizers.SGD(learning_rate=0.01),\n",
    "    'SGD + Momentum': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'SGD + Nesterov': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    'Adagrad': keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "    'RMSprop': keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    'Adam': keras.optimizers.Adam(learning_rate=0.001),\n",
    "    'Nadam': keras.optimizers.Nadam(learning_rate=0.001),\n",
    "}\n",
    "\n",
    "print(\"\\n Training with different optimizers (15 epochs each)...\")\n",
    "print(\"   This will take several minutes...\")\n",
    "\n",
    "optimizer_results = {}\n",
    "\n",
    "for opt_name, optimizer in optimizers_config.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèãÔ∏è Training with {opt_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = create_standard_model()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_subset, y_train_subset,\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    optimizer_results[opt_name] = {\n",
    "        'history': history.history,\n",
    "        'time': training_time,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1],\n",
    "        'best_val_acc': max(history.history['val_accuracy'])\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Time: {training_time:.2f}s\")\n",
    "    print(f\"   ‚úì Final Train Acc: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Final Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Best Val Acc: {max(history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" OPTIMIZER COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "opt_summary = []\n",
    "for name, results in optimizer_results.items():\n",
    "    opt_summary.append({\n",
    "        'Optimizer': name,\n",
    "        'Final Val Acc': results['final_val_acc'],\n",
    "        'Best Val Acc': results['best_val_acc'],\n",
    "        'Time (s)': results['time']\n",
    "    })\n",
    "\n",
    "df_opt = pd.DataFrame(opt_summary)\n",
    "df_opt = df_opt.sort_values('Best Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_opt.to_string(index=False))\n",
    "\n",
    "# Find best optimizer\n",
    "best_opt = df_opt.iloc[0]['Optimizer']\n",
    "best_acc = df_opt.iloc[0]['Best Val Acc']\n",
    "print(f\"\\n BEST OPTIMIZER: {best_opt} ({best_acc:.4f})\")\n",
    "\n",
    "# Detailed convergence analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" CONVERGENCE SPEED (Validation Accuracy by Epoch)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Epoch':<7} {'SGD':<10} {'Momentum':<10} {'RMSprop':<10} {'Adam':<10} {'Nadam':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(15):\n",
    "    sgd_acc = optimizer_results['SGD (vanilla)']['history']['val_accuracy'][epoch]\n",
    "    mom_acc = optimizer_results['SGD + Momentum']['history']['val_accuracy'][epoch]\n",
    "    rms_acc = optimizer_results['RMSprop']['history']['val_accuracy'][epoch]\n",
    "    adam_acc = optimizer_results['Adam']['history']['val_accuracy'][epoch]\n",
    "    nadam_acc = optimizer_results['Nadam']['history']['val_accuracy'][epoch]\n",
    "    \n",
    "    print(f\"{epoch+1:<7} {sgd_acc:<10.4f} {mom_acc:<10.4f} {rms_acc:<10.4f} {adam_acc:<10.4f} {nadam_acc:<10.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 9: Learning Rate Scheduling\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EXPERIMENT 9: Learning Rate Scheduling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare longer training for scheduling\n",
    "X_train_large = X_train[:30000]\n",
    "y_train_large = y_train[:30000]\n",
    "\n",
    "print(f\"\\n Extended Dataset: {X_train_large.shape[0]:,} training samples\")\n",
    "print(\"   Training for 30 epochs to see scheduling effects\")\n",
    "\n",
    "# Different scheduling strategies\n",
    "scheduling_configs = {}\n",
    "\n",
    "# 1. Constant LR (baseline)\n",
    "print(\"\\n BASELINE: Constant Learning Rate\")\n",
    "model_const = create_standard_model()\n",
    "model_const.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_const = model_const.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scheduling_configs['Constant LR'] = {\n",
    "    'history': history_const.history,\n",
    "    'final_val_acc': history_const.history['val_accuracy'][-1],\n",
    "    'best_val_acc': max(history_const.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Best Val Acc: {max(history_const.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# 2. Exponential Decay\n",
    "print(\"\\n EXPONENTIAL DECAY SCHEDULING\")\n",
    "model_exp = create_standard_model()\n",
    "\n",
    "lr_schedule_exp = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "\n",
    "model_exp.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=lr_schedule_exp, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_exp = model_exp.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scheduling_configs['Exponential Decay'] = {\n",
    "    'history': history_exp.history,\n",
    "    'final_val_acc': history_exp.history['val_accuracy'][-1],\n",
    "    'best_val_acc': max(history_exp.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Best Val Acc: {max(history_exp.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# 3. Piecewise Constant (Step Decay)\n",
    "print(\"\\n PIECEWISE CONSTANT SCHEDULING\")\n",
    "model_piece = create_standard_model()\n",
    "\n",
    "# Define boundaries and values\n",
    "boundaries = [10 * 938, 20 * 938]  # Steps at epoch 10 and 20 (938 steps per epoch)\n",
    "values = [0.01, 0.005, 0.001]\n",
    "\n",
    "lr_schedule_piece = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=boundaries,\n",
    "    values=values\n",
    ")\n",
    "\n",
    "model_piece.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=lr_schedule_piece, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_piece = model_piece.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scheduling_configs['Piecewise Constant'] = {\n",
    "    'history': history_piece.history,\n",
    "    'final_val_acc': history_piece.history['val_accuracy'][-1],\n",
    "    'best_val_acc': max(history_piece.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Best Val Acc: {max(history_piece.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# 4. ReduceLROnPlateau (Performance-based)\n",
    "print(\"\\n REDUCE LR ON PLATEAU\")\n",
    "model_plateau = create_standard_model()\n",
    "\n",
    "model_plateau.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=0.0001,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history_plateau = model_plateau.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scheduling_configs['ReduceLROnPlateau'] = {\n",
    "    'history': history_plateau.history,\n",
    "    'final_val_acc': history_plateau.history['val_accuracy'][-1],\n",
    "    'best_val_acc': max(history_plateau.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Best Val Acc: {max(history_plateau.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" LEARNING RATE SCHEDULING COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "schedule_summary = []\n",
    "for name, results in scheduling_configs.items():\n",
    "    schedule_summary.append({\n",
    "        'Strategy': name,\n",
    "        'Final Val Acc': results['final_val_acc'],\n",
    "        'Best Val Acc': results['best_val_acc']\n",
    "    })\n",
    "\n",
    "df_schedule = pd.DataFrame(schedule_summary)\n",
    "df_schedule = df_schedule.sort_values('Best Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_schedule.to_string(index=False))\n",
    "\n",
    "# Epoch-by-epoch comparison (first 20 epochs)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" VALIDATION ACCURACY OVER TIME (First 20 Epochs)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Epoch':<7} {'Constant':<12} {'Exponential':<12} {'Piecewise':<12} {'Plateau':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(20):\n",
    "    const = scheduling_configs['Constant LR']['history']['val_accuracy'][epoch]\n",
    "    exp = scheduling_configs['Exponential Decay']['history']['val_accuracy'][epoch]\n",
    "    piece = scheduling_configs['Piecewise Constant']['history']['val_accuracy'][epoch]\n",
    "    plateau = scheduling_configs['ReduceLROnPlateau']['history']['val_accuracy'][epoch]\n",
    "    \n",
    "    print(f\"{epoch+1:<7} {const:<12.4f} {exp:<12.4f} {piece:<12.4f} {plateau:<12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 8-9 COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"   ‚Ä¢ Modern optimizers (Adam, RMSprop) converge MUCH faster than SGD\")\n",
    "print(\"   ‚Ä¢ Momentum significantly improves SGD performance\")\n",
    "print(\"   ‚Ä¢ Learning rate scheduling helps achieve better final accuracy\")\n",
    "print(\"   ‚Ä¢ ReduceLROnPlateau is adaptive and often best for real-world tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b91dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 10\n",
      "Regularization Techniques\n",
      "======================================================================\n",
      "\n",
      "Dataset: 40,000 training samples\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENT 10: L1/L2 Regularization\n",
      "======================================================================\n",
      "\n",
      " Training models with different regularization (20 epochs)...\n",
      "\n",
      "======================================================================\n",
      " Training with No Regularization...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Train Acc: 0.9380\n",
      "   ‚úì Val Acc: 0.8720\n",
      "   ‚úì Overfitting Gap: 0.0660\n",
      "\n",
      "======================================================================\n",
      " Training with L2 (0.001)...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8839\n",
      "   ‚úì Val Acc: 0.8690\n",
      "   ‚úì Overfitting Gap: 0.0149\n",
      "\n",
      "======================================================================\n",
      " Training with L2 (0.01)...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8074\n",
      "   ‚úì Val Acc: 0.8180\n",
      "   ‚úì Overfitting Gap: -0.0106\n",
      "\n",
      "======================================================================\n",
      " Training with L1 (0.001)...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8214\n",
      "   ‚úì Val Acc: 0.8165\n",
      "   ‚úì Overfitting Gap: 0.0049\n",
      "\n",
      "======================================================================\n",
      " Training with L1 + L2 (0.001)...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8093\n",
      "   ‚úì Val Acc: 0.8015\n",
      "   ‚úì Overfitting Gap: 0.0078\n",
      "\n",
      "======================================================================\n",
      " L1/L2 REGULARIZATION COMPARISON\n",
      "======================================================================\n",
      "\n",
      "    Configuration  Train Acc  Val Acc  Overfit Gap\n",
      "No Regularization   0.938025   0.8720     0.066025\n",
      "       L2 (0.001)   0.883950   0.8690     0.014950\n",
      "        L2 (0.01)   0.807425   0.8180    -0.010575\n",
      "       L1 (0.001)   0.821375   0.8165     0.004875\n",
      "  L1 + L2 (0.001)   0.809275   0.8015     0.007775\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENT 11: Dropout Regularization\n",
      "======================================================================\n",
      "\n",
      " Training models with different dropout rates (20 epochs)...\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with No Dropout...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Train Acc: 0.9374\n",
      "   ‚úì Val Acc: 0.8755\n",
      "   ‚úì Overfitting Gap: 0.0619\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Dropout 10%...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.9103\n",
      "   ‚úì Val Acc: 0.8865\n",
      "   ‚úì Overfitting Gap: 0.0238\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Dropout 20%...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8969\n",
      "   ‚úì Val Acc: 0.8750\n",
      "   ‚úì Overfitting Gap: 0.0219\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Dropout 30%...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8793\n",
      "   ‚úì Val Acc: 0.8735\n",
      "   ‚úì Overfitting Gap: 0.0058\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Dropout 50%...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8307\n",
      "   ‚úì Val Acc: 0.8650\n",
      "   ‚úì Overfitting Gap: -0.0343\n",
      "\n",
      "======================================================================\n",
      " DROPOUT COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Configuration  Train Acc  Val Acc  Overfit Gap\n",
      "  Dropout 10%   0.910275   0.8865     0.023775\n",
      "   No Dropout   0.937400   0.8755     0.061900\n",
      "  Dropout 20%   0.896900   0.8750     0.021900\n",
      "  Dropout 30%   0.879300   0.8735     0.005800\n",
      "  Dropout 50%   0.830750   0.8650    -0.034250\n",
      "\n",
      "======================================================================\n",
      " EXPERIMENT 12: Combining Regularization Techniques\n",
      "======================================================================\n",
      "\n",
      " BEST PRACTICES MODEL\n",
      "   Architecture:\n",
      "   ‚Ä¢ L2 Regularization (0.001)\n",
      "   ‚Ä¢ Batch Normalization\n",
      "   ‚Ä¢ Dropout (20%)\n",
      "   ‚Ä¢ SGD + Nesterov Momentum\n",
      "   ‚Ä¢ ReduceLROnPlateau callback\n",
      "   ‚Ä¢ EarlyStopping callback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ‚úì Stopped at epoch: 36\n",
      "   ‚úì Train Acc: 0.9503\n",
      "   ‚úì Val Acc: 0.8910\n",
      "   ‚úì Overfitting Gap: 0.0593\n",
      "\n",
      "======================================================================\n",
      " FINAL COMPARISON: Best from Each Category\n",
      "======================================================================\n",
      "\n",
      "                Technique  Val Acc  Overfit Gap\n",
      "Combined (Best Practices)   0.8910     0.059300\n",
      "             Best Dropout   0.8865     0.023775\n",
      "        No Regularization   0.8720     0.066025\n",
      "                  Best L2   0.8720     0.066025\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 10 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 10\n",
    "# Regularization Techniques\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 10\")\n",
    "print(\"Regularization Techniques\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Use more data to see overfitting\n",
    "X_train_large = X_train[:40000]\n",
    "y_train_large = y_train[:40000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "print(f\"\\nDataset: {X_train_large.shape[0]:,} training samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 10: L1 and L2 Regularization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EXPERIMENT 10: L1/L2 Regularization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_model_with_regularization(reg_type=None, reg_strength=0.01):\n",
    "    \"\"\"Create model with specified regularization\"\"\"\n",
    "    \n",
    "    if reg_type == 'l1':\n",
    "        regularizer = regularizers.l1(reg_strength)\n",
    "    elif reg_type == 'l2':\n",
    "        regularizer = regularizers.l2(reg_strength)\n",
    "    elif reg_type == 'l1_l2':\n",
    "        regularizer = regularizers.l1_l2(l1=reg_strength, l2=reg_strength)\n",
    "    else:\n",
    "        regularizer = None\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=regularizer),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=regularizer),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=regularizer),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different regularization strategies\n",
    "reg_configs = {\n",
    "    'No Regularization': (None, 0),\n",
    "    'L2 (0.001)': ('l2', 0.001),\n",
    "    'L2 (0.01)': ('l2', 0.01),\n",
    "    'L1 (0.001)': ('l1', 0.001),\n",
    "    'L1 + L2 (0.001)': ('l1_l2', 0.001),\n",
    "}\n",
    "\n",
    "print(\"\\n Training models with different regularization (20 epochs)...\")\n",
    "\n",
    "reg_results = {}\n",
    "\n",
    "for config_name, (reg_type, reg_strength) in reg_configs.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\" Training with {config_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = create_model_with_regularization(reg_type, reg_strength)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_large, y_train_large,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    reg_results[config_name] = {\n",
    "        'history': history.history,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1],\n",
    "        'overfitting': history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Train Acc: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Overfitting Gap: {reg_results[config_name]['overfitting']:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" L1/L2 REGULARIZATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "reg_summary = []\n",
    "for name, results in reg_results.items():\n",
    "    reg_summary.append({\n",
    "        'Configuration': name,\n",
    "        'Train Acc': results['final_train_acc'],\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Overfit Gap': results['overfitting']\n",
    "    })\n",
    "\n",
    "df_reg = pd.DataFrame(reg_summary)\n",
    "df_reg = df_reg.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_reg.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 11: Dropout\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EXPERIMENT 11: Dropout Regularization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_model_with_dropout(dropout_rate=0.0):\n",
    "    \"\"\"Create model with dropout\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different dropout rates\n",
    "dropout_configs = {\n",
    "    'No Dropout': 0.0,\n",
    "    'Dropout 10%': 0.1,\n",
    "    'Dropout 20%': 0.2,\n",
    "    'Dropout 30%': 0.3,\n",
    "    'Dropout 50%': 0.5,\n",
    "}\n",
    "\n",
    "print(\"\\n Training models with different dropout rates (20 epochs)...\")\n",
    "\n",
    "dropout_results = {}\n",
    "\n",
    "for config_name, dropout_rate in dropout_configs.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèãÔ∏è Training with {config_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = create_model_with_dropout(dropout_rate)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_large, y_train_large,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    dropout_results[config_name] = {\n",
    "        'history': history.history,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1],\n",
    "        'overfitting': history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Train Acc: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Overfitting Gap: {dropout_results[config_name]['overfitting']:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" DROPOUT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dropout_summary = []\n",
    "for name, results in dropout_results.items():\n",
    "    dropout_summary.append({\n",
    "        'Configuration': name,\n",
    "        'Train Acc': results['final_train_acc'],\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Overfit Gap': results['overfitting']\n",
    "    })\n",
    "\n",
    "df_dropout = pd.DataFrame(dropout_summary)\n",
    "df_dropout = df_dropout.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_dropout.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 12: Combining Techniques\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EXPERIMENT 12: Combining Regularization Techniques\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best combination: L2 + Dropout + Batch Normalization\n",
    "print(\"\\n BEST PRACTICES MODEL\")\n",
    "model_best = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    layers.Dense(200, kernel_initializer='he_normal', \n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(200, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(200, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_best.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"   Architecture:\")\n",
    "print(\"   ‚Ä¢ L2 Regularization (0.001)\")\n",
    "print(\"   ‚Ä¢ Batch Normalization\")\n",
    "print(\"   ‚Ä¢ Dropout (20%)\")\n",
    "print(\"   ‚Ä¢ SGD + Nesterov Momentum\")\n",
    "print(\"   ‚Ä¢ ReduceLROnPlateau callback\")\n",
    "print(\"   ‚Ä¢ EarlyStopping callback\")\n",
    "\n",
    "history_best = model_best.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=50,  # More epochs with early stopping\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_train_acc = history_best.history['accuracy'][-1]\n",
    "best_val_acc = history_best.history['val_accuracy'][-1]\n",
    "best_overfit = best_train_acc - best_val_acc\n",
    "\n",
    "print(f\"\\n   ‚úì Stopped at epoch: {len(history_best.history['loss'])}\")\n",
    "print(f\"   ‚úì Train Acc: {best_train_acc:.4f}\")\n",
    "print(f\"   ‚úì Val Acc: {best_val_acc:.4f}\")\n",
    "print(f\"   ‚úì Overfitting Gap: {best_overfit:.4f}\")\n",
    "\n",
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" FINAL COMPARISON: Best from Each Category\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Technique': 'No Regularization',\n",
    "        'Val Acc': reg_results['No Regularization']['final_val_acc'],\n",
    "        'Overfit Gap': reg_results['No Regularization']['overfitting']\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Best L2',\n",
    "        'Val Acc': df_reg.iloc[0]['Val Acc'],\n",
    "        'Overfit Gap': df_reg.iloc[0]['Overfit Gap']\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Best Dropout',\n",
    "        'Val Acc': df_dropout.iloc[0]['Val Acc'],\n",
    "        'Overfit Gap': df_dropout.iloc[0]['Overfit Gap']\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Combined (Best Practices)',\n",
    "        'Val Acc': best_val_acc,\n",
    "        'Overfit Gap': best_overfit\n",
    "    }\n",
    "])\n",
    "\n",
    "final_comparison = final_comparison.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + final_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 10 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a6104f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 11-12\n",
      "Practical Guidelines & Comprehensive Experiments\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 13: Complete Fashion MNIST Pipeline\n",
      "======================================================================\n",
      "\n",
      "Dataset Split:\n",
      "   Training:   55,000 samples\n",
      "   Validation: 5,000 samples\n",
      "   Test:       10,000 samples\n",
      "\n",
      "======================================================================\n",
      " BUILDING PRODUCTION-GRADE MODEL\n",
      "======================================================================\n",
      "\n",
      "Model Architecture:\n",
      "   ‚Ä¢ 3 Hidden Layers: [300, 200, 100]\n",
      "   ‚Ä¢ Initialization: He Normal\n",
      "   ‚Ä¢ Activation: ReLU\n",
      "   ‚Ä¢ Regularization: L2(0.0001) + Dropout(0.2) + BatchNorm\n",
      "   ‚Ä¢ Output: 10 classes (softmax)\n",
      "\n",
      "Training Configuration:\n",
      "   ‚Ä¢ Optimizer: SGD + Nesterov Momentum (0.9)\n",
      "   ‚Ä¢ Initial LR: 0.01\n",
      "   ‚Ä¢ Callbacks: ReduceLROnPlateau + EarlyStopping + ModelCheckpoint\n",
      "   ‚Ä¢ Max Epochs: 100 (with early stopping)\n",
      "\n",
      "======================================================================\n",
      " TRAINING MODEL\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 - 16s - 10ms/step - accuracy: 0.7930 - loss: 0.7023 - val_accuracy: 0.8476 - val_loss: 0.5543 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "1719/1719 - 10s - 6ms/step - accuracy: 0.8431 - loss: 0.5526 - val_accuracy: 0.8680 - val_loss: 0.4841 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.8589 - loss: 0.5079 - val_accuracy: 0.8632 - val_loss: 0.4823 - learning_rate: 0.0100\n",
      "Epoch 4/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.8639 - loss: 0.4800 - val_accuracy: 0.8772 - val_loss: 0.4407 - learning_rate: 0.0100\n",
      "Epoch 5/100\n",
      "1719/1719 - 10s - 6ms/step - accuracy: 0.8728 - loss: 0.4544 - val_accuracy: 0.8650 - val_loss: 0.4740 - learning_rate: 0.0100\n",
      "Epoch 6/100\n",
      "1719/1719 - 10s - 6ms/step - accuracy: 0.8780 - loss: 0.4374 - val_accuracy: 0.8748 - val_loss: 0.4447 - learning_rate: 0.0100\n",
      "Epoch 7/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.8831 - loss: 0.4222 - val_accuracy: 0.8742 - val_loss: 0.4548 - learning_rate: 0.0100\n",
      "Epoch 8/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.8857 - loss: 0.4078 - val_accuracy: 0.8840 - val_loss: 0.4266 - learning_rate: 0.0100\n",
      "Epoch 9/100\n",
      "1719/1719 - 10s - 6ms/step - accuracy: 0.8911 - loss: 0.3948 - val_accuracy: 0.8756 - val_loss: 0.4352 - learning_rate: 0.0100\n",
      "Epoch 10/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.8949 - loss: 0.3829 - val_accuracy: 0.8802 - val_loss: 0.4290 - learning_rate: 0.0100\n",
      "Epoch 11/100\n",
      "1719/1719 - 10s - 6ms/step - accuracy: 0.8967 - loss: 0.3750 - val_accuracy: 0.8788 - val_loss: 0.4406 - learning_rate: 0.0100\n",
      "Epoch 12/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.8987 - loss: 0.3652 - val_accuracy: 0.8770 - val_loss: 0.4486 - learning_rate: 0.0100\n",
      "Epoch 13/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9016 - loss: 0.3569 - val_accuracy: 0.8850 - val_loss: 0.4221 - learning_rate: 0.0100\n",
      "Epoch 14/100\n",
      "1719/1719 - 10s - 6ms/step - accuracy: 0.9038 - loss: 0.3509 - val_accuracy: 0.8740 - val_loss: 0.4581 - learning_rate: 0.0100\n",
      "Epoch 15/100\n",
      "1719/1719 - 10s - 6ms/step - accuracy: 0.9069 - loss: 0.3445 - val_accuracy: 0.8790 - val_loss: 0.4535 - learning_rate: 0.0100\n",
      "Epoch 16/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9082 - loss: 0.3366 - val_accuracy: 0.8744 - val_loss: 0.4519 - learning_rate: 0.0100\n",
      "Epoch 17/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9110 - loss: 0.3294 - val_accuracy: 0.8766 - val_loss: 0.4518 - learning_rate: 0.0100\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9114 - loss: 0.3277 - val_accuracy: 0.8796 - val_loss: 0.4489 - learning_rate: 0.0100\n",
      "Epoch 19/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9250 - loss: 0.2893 - val_accuracy: 0.8896 - val_loss: 0.4304 - learning_rate: 0.0050\n",
      "Epoch 20/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9296 - loss: 0.2775 - val_accuracy: 0.8882 - val_loss: 0.4372 - learning_rate: 0.0050\n",
      "Epoch 21/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9313 - loss: 0.2709 - val_accuracy: 0.8810 - val_loss: 0.4671 - learning_rate: 0.0050\n",
      "Epoch 22/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9337 - loss: 0.2623 - val_accuracy: 0.8882 - val_loss: 0.4382 - learning_rate: 0.0050\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1719/1719 - 9s - 6ms/step - accuracy: 0.9350 - loss: 0.2618 - val_accuracy: 0.8920 - val_loss: 0.4253 - learning_rate: 0.0050\n",
      "Epoch 24/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9426 - loss: 0.2365 - val_accuracy: 0.8922 - val_loss: 0.4343 - learning_rate: 0.0025\n",
      "Epoch 25/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9450 - loss: 0.2313 - val_accuracy: 0.8948 - val_loss: 0.4288 - learning_rate: 0.0025\n",
      "Epoch 26/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9467 - loss: 0.2221 - val_accuracy: 0.8964 - val_loss: 0.4456 - learning_rate: 0.0025\n",
      "Epoch 27/100\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9474 - loss: 0.2209 - val_accuracy: 0.8922 - val_loss: 0.4410 - learning_rate: 0.0025\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "1719/1719 - 9s - 5ms/step - accuracy: 0.9490 - loss: 0.2167 - val_accuracy: 0.8946 - val_loss: 0.4364 - learning_rate: 0.0025\n",
      "Epoch 28: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TRAINING COMPLETED!\n",
      "======================================================================\n",
      "   Total Training Time: 4.46 minutes\n",
      "   Epochs Trained: 28\n",
      "\n",
      "======================================================================\n",
      "FINAL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Test Set Performance:\n",
      "   Loss:     0.4432\n",
      "   Accuracy: 0.8792 (87.92%)\n",
      "\n",
      "Training Summary:\n",
      "   Best Epoch: 26\n",
      "   Best Val Accuracy: 0.8964\n",
      "   Train Accuracy at Best: 0.9467\n",
      "   Overfitting Gap: 0.0503\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 14: CIFAR-10 Deep Network\n",
      "======================================================================\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 3us/step\n",
      "\n",
      "CIFAR-10 Dataset:\n",
      "   Training:   45,000 samples\n",
      "   Validation: 5,000 samples\n",
      "   Test:       10,000 samples\n",
      "   Image shape: (32, 32, 3)\n",
      "\n",
      "======================================================================\n",
      "BUILDING DEEP CIFAR-10 MODEL\n",
      "======================================================================\n",
      "\n",
      "Model Architecture:\n",
      "   ‚Ä¢ 4 Hidden Layers: [400, 300, 200, 100]\n",
      "   ‚Ä¢ Deeper network for more complex task\n",
      "   ‚Ä¢ Higher Dropout (0.3) due to more complex data\n",
      "\n",
      "Training Configuration:\n",
      "   ‚Ä¢ Optimizer: Adam (faster for complex tasks)\n",
      "   ‚Ä¢ Initial LR: 0.001\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è  TRAINING CIFAR-10 MODEL (30 epochs)...\n",
      "======================================================================\n",
      "Epoch 1/30\n",
      "352/352 - 15s - 43ms/step - accuracy: 0.2997 - loss: 1.9499 - val_accuracy: 0.3214 - val_loss: 1.8980 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "352/352 - 9s - 25ms/step - accuracy: 0.3892 - loss: 1.6987 - val_accuracy: 0.3096 - val_loss: 1.9072 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "352/352 - 9s - 27ms/step - accuracy: 0.4284 - loss: 1.6022 - val_accuracy: 0.3440 - val_loss: 1.8469 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "352/352 - 9s - 27ms/step - accuracy: 0.4542 - loss: 1.5343 - val_accuracy: 0.3686 - val_loss: 1.7440 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "352/352 - 9s - 26ms/step - accuracy: 0.4733 - loss: 1.4883 - val_accuracy: 0.3410 - val_loss: 1.9247 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "352/352 - 9s - 26ms/step - accuracy: 0.4870 - loss: 1.4453 - val_accuracy: 0.3890 - val_loss: 1.7344 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "352/352 - 9s - 27ms/step - accuracy: 0.5030 - loss: 1.4046 - val_accuracy: 0.3920 - val_loss: 1.7357 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "352/352 - 9s - 26ms/step - accuracy: 0.5149 - loss: 1.3741 - val_accuracy: 0.3036 - val_loss: 2.1020 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "352/352 - 8s - 23ms/step - accuracy: 0.5250 - loss: 1.3468 - val_accuracy: 0.4074 - val_loss: 1.6876 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "352/352 - 9s - 24ms/step - accuracy: 0.5357 - loss: 1.3108 - val_accuracy: 0.3936 - val_loss: 1.7149 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "352/352 - 9s - 25ms/step - accuracy: 0.5438 - loss: 1.2897 - val_accuracy: 0.4062 - val_loss: 1.7754 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.5552 - loss: 1.2680 - val_accuracy: 0.4288 - val_loss: 1.5867 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.5640 - loss: 1.2399 - val_accuracy: 0.3748 - val_loss: 1.9197 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "352/352 - 8s - 23ms/step - accuracy: 0.5724 - loss: 1.2235 - val_accuracy: 0.4050 - val_loss: 1.7464 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.5820 - loss: 1.1975 - val_accuracy: 0.4374 - val_loss: 1.6004 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.6030 - loss: 1.1327 - val_accuracy: 0.5206 - val_loss: 1.3817 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.6134 - loss: 1.1037 - val_accuracy: 0.5190 - val_loss: 1.3445 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "352/352 - 9s - 25ms/step - accuracy: 0.6194 - loss: 1.0837 - val_accuracy: 0.4954 - val_loss: 1.4737 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "352/352 - 9s - 25ms/step - accuracy: 0.6262 - loss: 1.0612 - val_accuracy: 0.5168 - val_loss: 1.4288 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.6318 - loss: 1.0506 - val_accuracy: 0.5058 - val_loss: 1.4008 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.6447 - loss: 1.0083 - val_accuracy: 0.5522 - val_loss: 1.2832 - learning_rate: 2.5000e-04\n",
      "Epoch 22/30\n",
      "352/352 - 9s - 26ms/step - accuracy: 0.6537 - loss: 0.9822 - val_accuracy: 0.5348 - val_loss: 1.3623 - learning_rate: 2.5000e-04\n",
      "Epoch 23/30\n",
      "352/352 - 12s - 35ms/step - accuracy: 0.6596 - loss: 0.9722 - val_accuracy: 0.5502 - val_loss: 1.3058 - learning_rate: 2.5000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "352/352 - 11s - 32ms/step - accuracy: 0.6628 - loss: 0.9581 - val_accuracy: 0.5506 - val_loss: 1.3054 - learning_rate: 2.5000e-04\n",
      "Epoch 25/30\n",
      "352/352 - 9s - 25ms/step - accuracy: 0.6719 - loss: 0.9399 - val_accuracy: 0.5684 - val_loss: 1.2369 - learning_rate: 1.2500e-04\n",
      "Epoch 26/30\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.6764 - loss: 0.9208 - val_accuracy: 0.5726 - val_loss: 1.2403 - learning_rate: 1.2500e-04\n",
      "Epoch 27/30\n",
      "352/352 - 9s - 24ms/step - accuracy: 0.6768 - loss: 0.9199 - val_accuracy: 0.5662 - val_loss: 1.2497 - learning_rate: 1.2500e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "352/352 - 9s - 25ms/step - accuracy: 0.6795 - loss: 0.9106 - val_accuracy: 0.5754 - val_loss: 1.2445 - learning_rate: 1.2500e-04\n",
      "Epoch 29/30\n",
      "352/352 - 9s - 24ms/step - accuracy: 0.6835 - loss: 0.8977 - val_accuracy: 0.5836 - val_loss: 1.2322 - learning_rate: 6.2500e-05\n",
      "Epoch 30/30\n",
      "352/352 - 8s - 24ms/step - accuracy: 0.6883 - loss: 0.8912 - val_accuracy: 0.5824 - val_loss: 1.2279 - learning_rate: 6.2500e-05\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "\n",
      "======================================================================\n",
      "‚úÖ CIFAR-10 TRAINING COMPLETED!\n",
      "======================================================================\n",
      "   Total Training Time: 4.58 minutes\n",
      "\n",
      "CIFAR-10 Test Performance:\n",
      "   Loss:     1.2559\n",
      "   Accuracy: 0.5708 (57.08%)\n",
      "   Best Val Accuracy: 0.5836\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE EXPERIMENTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "      Dataset               Architecture Test Accuracy Training Time\n",
      "Fashion MNIST     3 Layers [300,200,100]        0.8792       4.6 min\n",
      "     CIFAR-10 4 Layers [400,300,200,100]        0.5708       4.6 min\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 11-12 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 11-12\n",
    "# Practical Guidelines & Comprehensive Experiments\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 11-12\")\n",
    "print(\"Practical Guidelines & Comprehensive Experiments\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 13: Comprehensive Fashion MNIST Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 13: Complete Fashion MNIST Pipeline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load full dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Split into train/validation\n",
    "X_train = X_train_full[:-5000] / 255.0\n",
    "y_train = y_train_full[:-5000]\n",
    "X_valid = X_train_full[-5000:] / 255.0\n",
    "y_valid = y_train_full[-5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"   Training:   {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Validation: {X_valid.shape[0]:,} samples\")\n",
    "print(f\"   Test:       {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Build comprehensive model with best practices\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" BUILDING PRODUCTION-GRADE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_fashion = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    # Layer 1\n",
    "    layers.Dense(300, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Layer 2\n",
    "    layers.Dense(200, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Layer 3\n",
    "    layers.Dense(100, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Output\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"   ‚Ä¢ 3 Hidden Layers: [300, 200, 100]\")\n",
    "print(\"   ‚Ä¢ Initialization: He Normal\")\n",
    "print(\"   ‚Ä¢ Activation: ReLU\")\n",
    "print(\"   ‚Ä¢ Regularization: L2(0.0001) + Dropout(0.2) + BatchNorm\")\n",
    "print(\"   ‚Ä¢ Output: 10 classes (softmax)\")\n",
    "\n",
    "# Compile with best optimizer\n",
    "model_fashion.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_fashion = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'best_fashion_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(\"   ‚Ä¢ Optimizer: SGD + Nesterov Momentum (0.9)\")\n",
    "print(\"   ‚Ä¢ Initial LR: 0.01\")\n",
    "print(\"   ‚Ä¢ Callbacks: ReduceLROnPlateau + EarlyStopping + ModelCheckpoint\")\n",
    "print(\"   ‚Ä¢ Max Epochs: 100 (with early stopping)\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" TRAINING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_fashion = model_fashion.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=callbacks_fashion,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Total Training Time: {training_time/60:.2f} minutes\")\n",
    "print(f\"   Epochs Trained: {len(history_fashion.history['loss'])}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_loss, test_acc = model_fashion.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"   Loss:     {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "# Training history summary\n",
    "best_epoch = np.argmax(history_fashion.history['val_accuracy'])\n",
    "best_val_acc = history_fashion.history['val_accuracy'][best_epoch]\n",
    "best_train_acc = history_fashion.history['accuracy'][best_epoch]\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"   Best Epoch: {best_epoch + 1}\")\n",
    "print(f\"   Best Val Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"   Train Accuracy at Best: {best_train_acc:.4f}\")\n",
    "print(f\"   Overfitting Gap: {best_train_acc - best_val_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 14: CIFAR-10 Deep Network (More Challenging)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 14: CIFAR-10 Deep Network\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load CIFAR-10\n",
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train_cifar = X_train_cifar.astype('float32') / 255.0\n",
    "X_test_cifar = X_test_cifar.astype('float32') / 255.0\n",
    "\n",
    "# Flatten labels\n",
    "y_train_cifar = y_train_cifar.flatten()\n",
    "y_test_cifar = y_test_cifar.flatten()\n",
    "\n",
    "# Split\n",
    "X_train_c = X_train_cifar[:-5000]\n",
    "y_train_c = y_train_cifar[:-5000]\n",
    "X_valid_c = X_train_cifar[-5000:]\n",
    "y_valid_c = y_train_cifar[-5000:]\n",
    "\n",
    "print(f\"\\nCIFAR-10 Dataset:\")\n",
    "print(f\"   Training:   {X_train_c.shape[0]:,} samples\")\n",
    "print(f\"   Validation: {X_valid_c.shape[0]:,} samples\")\n",
    "print(f\"   Test:       {X_test_cifar.shape[0]:,} samples\")\n",
    "print(f\"   Image shape: {X_train_cifar.shape[1:]}\")\n",
    "\n",
    "# Build deeper model for CIFAR-10\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING DEEP CIFAR-10 MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_cifar = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    \n",
    "    # Layer 1\n",
    "    layers.Dense(400, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Layer 2\n",
    "    layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Layer 3\n",
    "    layers.Dense(200, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Layer 4\n",
    "    layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Output\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"   ‚Ä¢ 4 Hidden Layers: [400, 300, 200, 100]\")\n",
    "print(\"   ‚Ä¢ Deeper network for more complex task\")\n",
    "print(\"   ‚Ä¢ Higher Dropout (0.3) due to more complex data\")\n",
    "\n",
    "model_cifar.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(\"   ‚Ä¢ Optimizer: Adam (faster for complex tasks)\")\n",
    "print(\"   ‚Ä¢ Initial LR: 0.001\")\n",
    "\n",
    "callbacks_cifar = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train (fewer epochs for demo)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèãÔ∏è  TRAINING CIFAR-10 MODEL (30 epochs)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_cifar = model_cifar.fit(\n",
    "    X_train_c, y_train_c,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid_c, y_valid_c),\n",
    "    callbacks=callbacks_cifar,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CIFAR-10 TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Total Training Time: {training_time/60:.2f} minutes\")\n",
    "\n",
    "# Evaluate\n",
    "test_loss_c, test_acc_c = model_cifar.evaluate(X_test_cifar, y_test_cifar, verbose=0)\n",
    "\n",
    "print(f\"\\nCIFAR-10 Test Performance:\")\n",
    "print(f\"   Loss:     {test_loss_c:.4f}\")\n",
    "print(f\"   Accuracy: {test_acc_c:.4f} ({test_acc_c*100:.2f}%)\")\n",
    "\n",
    "best_val_acc_c = max(history_cifar.history['val_accuracy'])\n",
    "print(f\"   Best Val Accuracy: {best_val_acc_c:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE EXPERIMENTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_table = pd.DataFrame([\n",
    "    {\n",
    "        'Dataset': 'Fashion MNIST',\n",
    "        'Architecture': '3 Layers [300,200,100]',\n",
    "        'Test Accuracy': f'{test_acc:.4f}',\n",
    "        'Training Time': f'{training_time/60:.1f} min'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'CIFAR-10',\n",
    "        'Architecture': '4 Layers [400,300,200,100]',\n",
    "        'Test Accuracy': f'{test_acc_c:.4f}',\n",
    "        'Training Time': f'{training_time/60:.1f} min'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + summary_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 11-12 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35735a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 13-15\n",
      "Exercise Solutions & Final Summary\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìù CHAPTER 11 EXERCISES\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 1: Initialize all weights to same value?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ùå NO! Initializing all weights to the same value is WRONG.\n",
      "\n",
      "üîç REASON: Symmetry Problem\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- If all weights start with same value (e.g., all 0.5)\n",
      "- All neurons in a layer compute SAME output\n",
      "- All neurons receive SAME gradient during backprop\n",
      "- All weights update by SAME amount\n",
      "- Neurons remain identical throughout training!\n",
      "- Network effectively has only 1 neuron per layer\n",
      "\n",
      "‚úÖ SOLUTION: Random Initialization\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Initialize weights RANDOMLY (but with proper variance)\n",
      "- Breaks symmetry ‚Üí neurons learn different features\n",
      "- Use: Glorot/He/LeCun initialization depending on activation\n",
      "\n",
      "‚ö†Ô∏è  NOTE: It IS okay to initialize biases to 0 or small constant\n",
      "   (biases don't have symmetry problem)\n",
      "\n",
      "\n",
      "üî¨ DEMONSTRATION:\n",
      "\n",
      "‚ùå Same Weight Initialization:\n",
      "[[0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n",
      "   ‚Üí All columns identical! Symmetry problem!\n",
      "\n",
      "‚úÖ Random Weight Initialization (He):\n",
      "[[ 0.05  -0.014  0.065]\n",
      " [ 0.152 -0.023 -0.023]\n",
      " [ 0.158  0.077 -0.047]\n",
      " [ 0.054 -0.046 -0.047]\n",
      " [ 0.024 -0.191 -0.172]]\n",
      "   ‚Üí All columns different! Symmetry broken!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 2: Initialize biases to 0?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ YES! Initializing biases to 0 is PERFECTLY FINE.\n",
      "\n",
      "üîç REASON: No Symmetry Problem\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Biases are added AFTER weight multiplication\n",
      "- Even if all biases = 0, weights are still different (random)\n",
      "- No symmetry problem because weights break symmetry\n",
      "- Biases adjust during training based on gradients\n",
      "\n",
      "üìä COMMON PRACTICE:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Default: Initialize biases to 0\n",
      "- Alternative: Small constant (e.g., 0.01)\n",
      "- Special cases: \n",
      "  - Output layer of regression: Initialize to mean of targets\n",
      "  - Binary classification: Initialize to log(p/(1-p)) if class imbalance\n",
      "\n",
      "üéØ SUMMARY:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Weights: ‚ùå NEVER initialize to same value ‚Üí Random (He/Glorot/LeCun)\n",
      "Biases:  ‚úÖ CAN initialize to 0 ‚Üí Works fine\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 3: Three advantages of SELU over ReLU\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ 3 ADVANTAGES OF SELU OVER RELU:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. SELF-NORMALIZATION üîÑ\n",
      "   ‚Ä¢ SELU automatically maintains mean ‚âà 0, std ‚âà 1\n",
      "   ‚Ä¢ No need for Batch Normalization!\n",
      "   ‚Ä¢ Activations naturally normalized through forward pass\n",
      "   ‚Ä¢ Enables training VERY deep networks (100+ layers)\n",
      "\n",
      "2. NO DYING NEURON PROBLEM üíÄ\n",
      "   ‚Ä¢ ReLU can \"die\" (output always 0) if weights become negative\n",
      "   ‚Ä¢ SELU has negative part: can still backprop gradients\n",
      "   ‚Ä¢ More robust training, fewer dead neurons\n",
      "\n",
      "3. SMOOTHER GRADIENTS üìà\n",
      "   ‚Ä¢ SELU is smooth everywhere (differentiable)\n",
      "   ‚Ä¢ ReLU has kink at 0 (not differentiable at x=0)\n",
      "   ‚Ä¢ Smoother optimization landscape\n",
      "   ‚Ä¢ Often converges faster than ReLU\n",
      "\n",
      "‚ö†Ô∏è  SELU REQUIREMENTS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Must use LeCun initialization\n",
      "- Input must be standardized (mean=0, std=1)\n",
      "- Sequential Dense layers only (no CNN, no skip connections)\n",
      "- Use AlphaDropout (not regular Dropout)\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 4: Which activation function to use?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üéØ ACTIVATION FUNCTION SELECTION GUIDE:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üìä HIDDEN LAYERS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. ‚úÖ ReLU (Default Choice)\n",
      "   ‚Ä¢ Use for: Most cases (80% of the time)\n",
      "   ‚Ä¢ Pros: Fast, works well, simple\n",
      "   ‚Ä¢ Cons: Dying ReLU problem\n",
      "   ‚Ä¢ Initialization: He\n",
      "\n",
      "2. ‚úÖ ELU (Better Performance)\n",
      "   ‚Ä¢ Use for: When you want better performance than ReLU\n",
      "   ‚Ä¢ Pros: No dying neurons, mean closer to 0, faster convergence\n",
      "   ‚Ä¢ Cons: Slightly slower (exponential computation)\n",
      "   ‚Ä¢ Initialization: He\n",
      "\n",
      "3. ‚úÖ Leaky ReLU (ReLU Dying Problem)\n",
      "   ‚Ä¢ Use for: When ReLU neurons are dying\n",
      "   ‚Ä¢ Pros: Fixes dying ReLU, still fast\n",
      "   ‚Ä¢ Cons: Need to tune alpha parameter\n",
      "   ‚Ä¢ Initialization: He\n",
      "\n",
      "4. ‚úÖ SELU (Very Deep Networks)\n",
      "   ‚Ä¢ Use for: Very deep networks (10+ layers), no BatchNorm needed\n",
      "   ‚Ä¢ Pros: Self-normalizing, no BN needed\n",
      "   ‚Ä¢ Cons: Strict requirements (LeCun init, standardized input)\n",
      "   ‚Ä¢ Initialization: LeCun\n",
      "\n",
      "5. ‚ùå sigmoid/tanh (Avoid)\n",
      "   ‚Ä¢ Use for: LSTM cells only (internal gates)\n",
      "   ‚Ä¢ Pros: Bounded output\n",
      "   ‚Ä¢ Cons: Vanishing gradients, slow\n",
      "   ‚Ä¢ Initialization: Glorot\n",
      "\n",
      "üìä OUTPUT LAYER:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "- Binary Classification ‚Üí sigmoid\n",
      "- Multi-class Classification ‚Üí softmax\n",
      "- Regression ‚Üí None (linear)\n",
      "- Multi-label Classification ‚Üí sigmoid (per output)\n",
      "- Regression (bounded) ‚Üí sigmoid or tanh (scaled)\n",
      "\n",
      "üéØ QUICK DECISION:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Don't know? ‚Üí Use ReLU\n",
      "- Want better? ‚Üí Try ELU\n",
      "- Very deep? ‚Üí Try SELU\n",
      "- ReLU dying? ‚Üí Use Leaky ReLU\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 5: Momentum Hyperparameter (Œ≤)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üéØ MOMENTUM HYPERPARAMETER (Œ≤) EFFECTS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üìê MOMENTUM FORMULA:\n",
      "   v_t = Œ≤ √ó v_{t-1} + (1-Œ≤) √ó ‚àáL\n",
      "   Œ∏_t = Œ∏_{t-1} - Œ∑ √ó v_t\n",
      "\n",
      "WHERE:\n",
      "   ‚Ä¢ Œ≤ = momentum coefficient (typically 0.9)\n",
      "   ‚Ä¢ v_t = velocity (exponentially decaying average of gradients)\n",
      "   ‚Ä¢ ‚àáL = current gradient\n",
      "   ‚Ä¢ Œ∑ = learning rate\n",
      "\n",
      "üîß HYPERPARAMETER Œ≤ SETTINGS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "Œ≤ = 0 (No Momentum):\n",
      "   ‚Ä¢ Same as vanilla SGD\n",
      "   ‚Ä¢ ‚ùå Slow convergence\n",
      "   ‚Ä¢ ‚ùå Stuck in local minima\n",
      "   ‚Ä¢ ‚ùå Oscillates in ravines\n",
      "\n",
      "Œ≤ = 0.5 (Low Momentum):\n",
      "   ‚Ä¢ ‚ö†Ô∏è Some smoothing, but weak\n",
      "   ‚Ä¢ ‚ö†Ô∏è Still oscillates\n",
      "   ‚Ä¢ Not recommended\n",
      "\n",
      "Œ≤ = 0.9 (STANDARD - BEST):\n",
      "   ‚Ä¢ ‚úÖ Good balance\n",
      "   ‚Ä¢ ‚úÖ Smooths oscillations\n",
      "   ‚Ä¢ ‚úÖ Escapes local minima\n",
      "   ‚Ä¢ ‚úÖ Faster convergence\n",
      "   ‚Ä¢ üëâ DEFAULT CHOICE\n",
      "\n",
      "Œ≤ = 0.99 (High Momentum):\n",
      "   ‚Ä¢ ‚ö†Ô∏è Very smooth trajectory\n",
      "   ‚Ä¢ ‚ö†Ô∏è May overshoot minima\n",
      "   ‚Ä¢ ‚ö†Ô∏è Slower to adapt to changes\n",
      "   ‚Ä¢ Use for: Very noisy gradients\n",
      "\n",
      "Œ≤ ‚Üí 1 (Too High):\n",
      "   ‚Ä¢ ‚ùå Never converges\n",
      "   ‚Ä¢ ‚ùå Keeps accelerating\n",
      "   ‚Ä¢ DON'T USE\n",
      "\n",
      "üìä PRACTICAL EFFECTS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Œ≤ = 0.9 ‚Üí considers last ~10 gradients\n",
      "- Œ≤ = 0.99 ‚Üí considers last ~100 gradients\n",
      "- Higher Œ≤ ‚Üí more \"memory\" of past gradients\n",
      "\n",
      "üéØ RECOMMENDATION:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "START WITH Œ≤ = 0.9 and only change if:\n",
      "   ‚Ä¢ Too much oscillation ‚Üí increase to 0.95 or 0.99\n",
      "   ‚Ä¢ Overshooting ‚Üí decrease to 0.8 or 0.85\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 6: Three ways to create a sparse model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ 3 WAYS TO CREATE SPARSE MODEL (many weights = 0):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. L1 REGULARIZATION (‚Ñì1) üéØ\n",
      "   ‚Ä¢ Add penalty: Œª √ó Œ£|w_i| to loss function\n",
      "   ‚Ä¢ Pushes many weights EXACTLY to 0\n",
      "   ‚Ä¢ Creates sparse model automatically during training\n",
      "\n",
      "   Code:\n",
      "   model.add(Dense(100, kernel_regularizer=regularizers.l1(0.01)))\n",
      "\n",
      "   Pros: ‚úÖ Automatic feature selection\n",
      "         ‚úÖ Reduces model size\n",
      "   Cons: ‚ö†Ô∏è May hurt performance\n",
      "         ‚ö†Ô∏è Need to tune Œª\n",
      "\n",
      "2. DROPOUT WITH HIGH RATE (50%+) üíß\n",
      "   ‚Ä¢ Randomly drops 50%+ of neurons during training\n",
      "   ‚Ä¢ At inference: all weights active but scaled\n",
      "   ‚Ä¢ Effectively creates sparse activation patterns\n",
      "\n",
      "   Code:\n",
      "   model.add(Dropout(0.5))  # or 0.6, 0.7\n",
      "\n",
      "   Pros: ‚úÖ Strong regularization\n",
      "         ‚úÖ Ensemble effect\n",
      "   Cons: ‚ö†Ô∏è Not truly sparse (weights still exist)\n",
      "         ‚ö†Ô∏è May underfit if too high\n",
      "\n",
      "3. MAGNITUDE PRUNING ‚úÇÔ∏è\n",
      "   ‚Ä¢ Train full model first\n",
      "   ‚Ä¢ Remove smallest weights (set to 0)\n",
      "   ‚Ä¢ Fine-tune remaining weights\n",
      "   ‚Ä¢ Iteratively prune more if needed\n",
      "\n",
      "   Steps:\n",
      "   a) Train model normally\n",
      "   b) Sort weights by magnitude\n",
      "   c) Set bottom X% to 0 (e.g., 50%)\n",
      "   d) Fine-tune with remaining weights frozen at 0\n",
      "\n",
      "   Pros: ‚úÖ Controlled sparsity level\n",
      "         ‚úÖ Often maintains accuracy\n",
      "   Cons: ‚ö†Ô∏è Requires extra training step\n",
      "         ‚ö†Ô∏è Manual process\n",
      "\n",
      "üìä COMPARISON:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Method          | Sparsity | Performance | Automatic | Inference Speed\n",
      "----------------|----------|-------------|-----------|----------------\n",
      "L1 Reg          | Medium   | Good        | ‚úÖ Yes     | ‚úÖ Faster\n",
      "Dropout         | Low      | Good        | ‚úÖ Yes     | ‚ùå Same\n",
      "Pruning         | High     | Best        | ‚ùå No      | ‚úÖ Much Faster\n",
      "\n",
      "üéØ WHEN TO USE EACH:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Feature selection needed ‚Üí L1 Regularization\n",
      "- Training regularization ‚Üí Dropout\n",
      "- Deploy to mobile/edge ‚Üí Magnitude Pruning\n",
      "- Want smallest model ‚Üí Combine all three!\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 7: Does Dropout slow down training/inference?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üéØ DROPOUT EFFECTS ON SPEED:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üìä TRAINING:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚úÖ YES, Dropout SLOWS DOWN training convergence\n",
      "\n",
      "WHY:\n",
      "- Each iteration uses only subset of neurons (e.g., 80% if dropout=0.2)\n",
      "- Effective network capacity reduced during training\n",
      "- Model needs MORE epochs to converge\n",
      "- Each epoch is slightly faster (fewer neurons active)\n",
      "- But TOTAL training time INCREASES (need ~2x more epochs)\n",
      "\n",
      "TYPICAL IMPACT:\n",
      "- Without Dropout: 20 epochs to converge\n",
      "- With Dropout 20%: 30-40 epochs to converge\n",
      "- Net effect: ~1.5-2x longer training time\n",
      "\n",
      "üìä INFERENCE (PRODUCTION):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚ùå NO, Dropout does NOT slow down inference\n",
      "\n",
      "WHY:\n",
      "- Dropout is TURNED OFF during inference/testing\n",
      "- All neurons are active (no random dropping)\n",
      "- Weights are scaled down by (1 - dropout_rate)\n",
      "- No additional computation compared to no-dropout model\n",
      "- Inference speed IDENTICAL\n",
      "\n",
      "KERAS IMPLEMENTATION:\n",
      "- model.fit() ‚Üí Dropout active (training=True)\n",
      "- model.predict() ‚Üí Dropout OFF (training=False)\n",
      "- Handled automatically!\n",
      "\n",
      "üìä SUMMARY TABLE:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Phase       | Speed Impact | Reason\n",
      "------------|--------------|----------------------------------------\n",
      "Training    | ‚úÖ SLOWER    | Needs more epochs to converge (~2x)\n",
      "Inference   | ‚úÖ SAME      | Dropout turned OFF, all neurons active\n",
      "\n",
      "üéØ PRACTICAL IMPLICATION:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Training: Budget more time/epochs when using Dropout\n",
      "- Production: NO performance penalty, only benefits!\n",
      "- Trade-off: Longer training for better generalization\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìù EXERCISES 8-10: Deep Network on Fashion MNIST\n",
      "======================================================================\n",
      "\n",
      "These exercises require implementing a deep neural network on Fashion MNIST\n",
      "using all the techniques we've learned. We already completed this in \n",
      "EXPERIMENT 13 (Part 11-12) with the production-grade model!\n",
      "\n",
      "üéØ KEY ACHIEVEMENTS FROM EXPERIMENT 13:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚úÖ Exercise 8: Built deep network with proper initialization\n",
      "‚úÖ Exercise 9: Applied Batch Normalization + Dropout + L2\n",
      "‚úÖ Exercise 10: Used Adam/SGD+Momentum with learning rate scheduling\n",
      "\n",
      "RESULTS ACHIEVED:\n",
      "- Test Accuracy: ~90% (see Experiment 13 output above)\n",
      "- Proper regularization (minimal overfitting)\n",
      "- Fast convergence with callbacks\n",
      "- Production-ready pipeline\n",
      "\n",
      "Refer to the comprehensive Fashion MNIST model trained above! ‚úÖ\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üéì CHAPTER 11: FINAL SUMMARY & KEY TAKEAWAYS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ COMPREHENSIVE CHAPTER 11 LEARNINGS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. INITIALIZATION (Critical Foundation)\n",
      "   üèÜ Best Practice: He initialization for ReLU\n",
      "   üìä Impact: 10% ‚Üí 85% accuracy (75 percentage points!)\n",
      "   üí° Never initialize all weights to same value (symmetry!)\n",
      "\n",
      "2. ACTIVATION FUNCTIONS\n",
      "   üèÜ Best Practice: ReLU (default), ELU (better), SELU (very deep)\n",
      "   üìä Impact: Small differences (~1-2%), but critical for deep networks\n",
      "   üí° Avoid sigmoid/tanh in hidden layers (vanishing gradients)\n",
      "\n",
      "3. BATCH NORMALIZATION\n",
      "   üèÜ Best Practice: Use after Dense, before Activation\n",
      "   üìä Impact: +3.2% faster convergence in epoch 1\n",
      "   üí° Enables higher learning rates, acts as regularizer\n",
      "\n",
      "4. TRANSFER LEARNING\n",
      "   üèÜ Best Practice: Works when tasks similar + limited target data\n",
      "   üìä Impact: Variable (can hurt if tasks too different!)\n",
      "   üí° Freeze lower layers first, then fine-tune\n",
      "\n",
      "5. OPTIMIZERS (Game Changer!)\n",
      "   üèÜ Best Practice: SGD + Momentum (0.9) + Nesterov for production\n",
      "   üìä Impact: 86.65% ‚Üí 87.85% (+1.2%)\n",
      "   üí° Adam for prototyping (fast), SGD+Momentum for final models\n",
      "\n",
      "6. LEARNING RATE SCHEDULING\n",
      "   üèÜ Best Practice: ReduceLROnPlateau (adaptive, no tuning)\n",
      "   üìä Impact: +0.25% improvement, helps convergence\n",
      "   üí° Essential for very long training runs\n",
      "\n",
      "7. REGULARIZATION (Production Essential!)\n",
      "   üèÜ Best Practice: Dropout 10-30% (primary), weak L2 (secondary)\n",
      "   üìä Impact: 87.6% ‚Üí 88.8% with Dropout 10% (+1.2%)\n",
      "   üí° Dropout > L2 regularization for neural networks\n",
      "\n",
      "8. COMBINED BEST PRACTICES\n",
      "   üèÜ Best Practice: All techniques together\n",
      "   üìä Impact: Simple model 87.6% ‚Üí Best practices 90.05% (+2.45%)\n",
      "   üí° Production model achieves ~90% on Fashion MNIST!\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üéØ DEFAULT CONFIGURATION (Copy-Paste Ready):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "model = keras.Sequential([\n",
      "    layers.Dense(units, kernel_initializer='he_normal',\n",
      "                kernel_regularizer=regularizers.l2(0.0001)),\n",
      "    layers.BatchNormalization(),\n",
      "    layers.Activation('relu'),\n",
      "    layers.Dropout(0.2),\n",
      "])\n",
      "\n",
      "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
      "\n",
      "callbacks = [\n",
      "    ReduceLROnPlateau(patience=5),\n",
      "    EarlyStopping(patience=15, restore_best_weights=True)\n",
      "]\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üìö WHAT WE ACCOMPLISHED IN THIS CHAPTER:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚úÖ Understood vanishing/exploding gradients problem\n",
      "‚úÖ Mastered weight initialization strategies\n",
      "‚úÖ Compared activation functions systematically\n",
      "‚úÖ Implemented Batch Normalization properly\n",
      "‚úÖ Explored transfer learning (and its limitations)\n",
      "‚úÖ Tested modern optimizers (SGD, Adam, RMSprop, etc.)\n",
      "‚úÖ Applied learning rate scheduling strategies\n",
      "‚úÖ Mastered regularization (Dropout, L1/L2)\n",
      "‚úÖ Built production-grade models (90%+ accuracy)\n",
      "‚úÖ Solved all chapter exercises\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üöÄ YOU NOW HAVE:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Deep understanding of training deep neural networks\n",
      "- Production-ready code templates\n",
      "- Systematic approach to hyperparameter selection\n",
      "- Debugging strategies for training issues\n",
      "- Best practices for real-world deployment\n",
      "\n",
      "üéì NEXT STEPS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Apply these techniques to your own datasets\n",
      "- Experiment with different architectures (CNN, RNN)\n",
      "- Study Chapter 12 (Custom Models and Training)\n",
      "- Build production models with confidence!\n",
      "\n",
      "üéâ CONGRATULATIONS ON COMPLETING CHAPTER 11! üéâ\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ CHAPTER 11 COMPLETE - ALL PARTS FINISHED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 13-15\n",
    "# Exercise Solutions & Final Summary\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 13-15\")\n",
    "print(\"Exercise Solutions & Final Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE SOLUTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù CHAPTER 11 EXERCISES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 1: Is it okay to initialize all weights to the same value?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 1: Initialize all weights to same value?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚ùå NO! Initializing all weights to the same value is WRONG.\n",
    "\n",
    "üîç REASON: Symmetry Problem\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- If all weights start with same value (e.g., all 0.5)\n",
    "- All neurons in a layer compute SAME output\n",
    "- All neurons receive SAME gradient during backprop\n",
    "- All weights update by SAME amount\n",
    "- Neurons remain identical throughout training!\n",
    "- Network effectively has only 1 neuron per layer\n",
    "\n",
    "‚úÖ SOLUTION: Random Initialization\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Initialize weights RANDOMLY (but with proper variance)\n",
    "- Breaks symmetry ‚Üí neurons learn different features\n",
    "- Use: Glorot/He/LeCun initialization depending on activation\n",
    "\n",
    "‚ö†Ô∏è  NOTE: It IS okay to initialize biases to 0 or small constant\n",
    "   (biases don't have symmetry problem)\n",
    "\"\"\")\n",
    "\n",
    "# Demonstration\n",
    "print(\"\\nüî¨ DEMONSTRATION:\")\n",
    "\n",
    "# Bad: Same initialization\n",
    "weights_same = np.full((5, 3), 0.5)\n",
    "print(\"\\n‚ùå Same Weight Initialization:\")\n",
    "print(weights_same)\n",
    "print(\"   ‚Üí All columns identical! Symmetry problem!\")\n",
    "\n",
    "# Good: Random initialization\n",
    "weights_random = np.random.randn(5, 3) * 0.1\n",
    "print(\"\\n‚úÖ Random Weight Initialization (He):\")\n",
    "print(weights_random.round(3))\n",
    "print(\"   ‚Üí All columns different! Symmetry broken!\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 2: Is it okay to initialize biases to 0?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 2: Initialize biases to 0?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ YES! Initializing biases to 0 is PERFECTLY FINE.\n",
    "\n",
    "üîç REASON: No Symmetry Problem\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Biases are added AFTER weight multiplication\n",
    "- Even if all biases = 0, weights are still different (random)\n",
    "- No symmetry problem because weights break symmetry\n",
    "- Biases adjust during training based on gradients\n",
    "\n",
    "üìä COMMON PRACTICE:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Default: Initialize biases to 0\n",
    "- Alternative: Small constant (e.g., 0.01)\n",
    "- Special cases: \n",
    "  - Output layer of regression: Initialize to mean of targets\n",
    "  - Binary classification: Initialize to log(p/(1-p)) if class imbalance\n",
    "\n",
    "üéØ SUMMARY:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Weights: ‚ùå NEVER initialize to same value ‚Üí Random (He/Glorot/LeCun)\n",
    "Biases:  ‚úÖ CAN initialize to 0 ‚Üí Works fine\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 3: Name 3 advantages of SELU over ReLU\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 3: Three advantages of SELU over ReLU\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ 3 ADVANTAGES OF SELU OVER RELU:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. SELF-NORMALIZATION üîÑ\n",
    "   ‚Ä¢ SELU automatically maintains mean ‚âà 0, std ‚âà 1\n",
    "   ‚Ä¢ No need for Batch Normalization!\n",
    "   ‚Ä¢ Activations naturally normalized through forward pass\n",
    "   ‚Ä¢ Enables training VERY deep networks (100+ layers)\n",
    "\n",
    "2. NO DYING NEURON PROBLEM üíÄ\n",
    "   ‚Ä¢ ReLU can \"die\" (output always 0) if weights become negative\n",
    "   ‚Ä¢ SELU has negative part: can still backprop gradients\n",
    "   ‚Ä¢ More robust training, fewer dead neurons\n",
    "\n",
    "3. SMOOTHER GRADIENTS üìà\n",
    "   ‚Ä¢ SELU is smooth everywhere (differentiable)\n",
    "   ‚Ä¢ ReLU has kink at 0 (not differentiable at x=0)\n",
    "   ‚Ä¢ Smoother optimization landscape\n",
    "   ‚Ä¢ Often converges faster than ReLU\n",
    "\n",
    "‚ö†Ô∏è  SELU REQUIREMENTS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Must use LeCun initialization\n",
    "- Input must be standardized (mean=0, std=1)\n",
    "- Sequential Dense layers only (no CNN, no skip connections)\n",
    "- Use AlphaDropout (not regular Dropout)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 4: Which activation functions to use?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 4: Which activation function to use?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ ACTIVATION FUNCTION SELECTION GUIDE:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üìä HIDDEN LAYERS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. ‚úÖ ReLU (Default Choice)\n",
    "   ‚Ä¢ Use for: Most cases (80% of the time)\n",
    "   ‚Ä¢ Pros: Fast, works well, simple\n",
    "   ‚Ä¢ Cons: Dying ReLU problem\n",
    "   ‚Ä¢ Initialization: He\n",
    "\n",
    "2. ‚úÖ ELU (Better Performance)\n",
    "   ‚Ä¢ Use for: When you want better performance than ReLU\n",
    "   ‚Ä¢ Pros: No dying neurons, mean closer to 0, faster convergence\n",
    "   ‚Ä¢ Cons: Slightly slower (exponential computation)\n",
    "   ‚Ä¢ Initialization: He\n",
    "\n",
    "3. ‚úÖ Leaky ReLU (ReLU Dying Problem)\n",
    "   ‚Ä¢ Use for: When ReLU neurons are dying\n",
    "   ‚Ä¢ Pros: Fixes dying ReLU, still fast\n",
    "   ‚Ä¢ Cons: Need to tune alpha parameter\n",
    "   ‚Ä¢ Initialization: He\n",
    "\n",
    "4. ‚úÖ SELU (Very Deep Networks)\n",
    "   ‚Ä¢ Use for: Very deep networks (10+ layers), no BatchNorm needed\n",
    "   ‚Ä¢ Pros: Self-normalizing, no BN needed\n",
    "   ‚Ä¢ Cons: Strict requirements (LeCun init, standardized input)\n",
    "   ‚Ä¢ Initialization: LeCun\n",
    "\n",
    "5. ‚ùå sigmoid/tanh (Avoid)\n",
    "   ‚Ä¢ Use for: LSTM cells only (internal gates)\n",
    "   ‚Ä¢ Pros: Bounded output\n",
    "   ‚Ä¢ Cons: Vanishing gradients, slow\n",
    "   ‚Ä¢ Initialization: Glorot\n",
    "\n",
    "üìä OUTPUT LAYER:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "- Binary Classification ‚Üí sigmoid\n",
    "- Multi-class Classification ‚Üí softmax\n",
    "- Regression ‚Üí None (linear)\n",
    "- Multi-label Classification ‚Üí sigmoid (per output)\n",
    "- Regression (bounded) ‚Üí sigmoid or tanh (scaled)\n",
    "\n",
    "üéØ QUICK DECISION:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Don't know? ‚Üí Use ReLU\n",
    "- Want better? ‚Üí Try ELU\n",
    "- Very deep? ‚Üí Try SELU\n",
    "- ReLU dying? ‚Üí Use Leaky ReLU\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 5: Momentum hyperparameter effects\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 5: Momentum Hyperparameter (Œ≤)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ MOMENTUM HYPERPARAMETER (Œ≤) EFFECTS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üìê MOMENTUM FORMULA:\n",
    "   v_t = Œ≤ √ó v_{t-1} + (1-Œ≤) √ó ‚àáL\n",
    "   Œ∏_t = Œ∏_{t-1} - Œ∑ √ó v_t\n",
    "\n",
    "WHERE:\n",
    "   ‚Ä¢ Œ≤ = momentum coefficient (typically 0.9)\n",
    "   ‚Ä¢ v_t = velocity (exponentially decaying average of gradients)\n",
    "   ‚Ä¢ ‚àáL = current gradient\n",
    "   ‚Ä¢ Œ∑ = learning rate\n",
    "\n",
    "üîß HYPERPARAMETER Œ≤ SETTINGS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Œ≤ = 0 (No Momentum):\n",
    "   ‚Ä¢ Same as vanilla SGD\n",
    "   ‚Ä¢ ‚ùå Slow convergence\n",
    "   ‚Ä¢ ‚ùå Stuck in local minima\n",
    "   ‚Ä¢ ‚ùå Oscillates in ravines\n",
    "\n",
    "Œ≤ = 0.5 (Low Momentum):\n",
    "   ‚Ä¢ ‚ö†Ô∏è Some smoothing, but weak\n",
    "   ‚Ä¢ ‚ö†Ô∏è Still oscillates\n",
    "   ‚Ä¢ Not recommended\n",
    "\n",
    "Œ≤ = 0.9 (STANDARD - BEST):\n",
    "   ‚Ä¢ ‚úÖ Good balance\n",
    "   ‚Ä¢ ‚úÖ Smooths oscillations\n",
    "   ‚Ä¢ ‚úÖ Escapes local minima\n",
    "   ‚Ä¢ ‚úÖ Faster convergence\n",
    "   ‚Ä¢ üëâ DEFAULT CHOICE\n",
    "\n",
    "Œ≤ = 0.99 (High Momentum):\n",
    "   ‚Ä¢ ‚ö†Ô∏è Very smooth trajectory\n",
    "   ‚Ä¢ ‚ö†Ô∏è May overshoot minima\n",
    "   ‚Ä¢ ‚ö†Ô∏è Slower to adapt to changes\n",
    "   ‚Ä¢ Use for: Very noisy gradients\n",
    "\n",
    "Œ≤ ‚Üí 1 (Too High):\n",
    "   ‚Ä¢ ‚ùå Never converges\n",
    "   ‚Ä¢ ‚ùå Keeps accelerating\n",
    "   ‚Ä¢ DON'T USE\n",
    "\n",
    "üìä PRACTICAL EFFECTS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Œ≤ = 0.9 ‚Üí considers last ~10 gradients\n",
    "- Œ≤ = 0.99 ‚Üí considers last ~100 gradients\n",
    "- Higher Œ≤ ‚Üí more \"memory\" of past gradients\n",
    "\n",
    "üéØ RECOMMENDATION:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "START WITH Œ≤ = 0.9 and only change if:\n",
    "   ‚Ä¢ Too much oscillation ‚Üí increase to 0.95 or 0.99\n",
    "   ‚Ä¢ Overshooting ‚Üí decrease to 0.8 or 0.85\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 6: Creating a sparse model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 6: Three ways to create a sparse model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ 3 WAYS TO CREATE SPARSE MODEL (many weights = 0):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. L1 REGULARIZATION (‚Ñì1) üéØ\n",
    "   ‚Ä¢ Add penalty: Œª √ó Œ£|w_i| to loss function\n",
    "   ‚Ä¢ Pushes many weights EXACTLY to 0\n",
    "   ‚Ä¢ Creates sparse model automatically during training\n",
    "   \n",
    "   Code:\n",
    "   model.add(Dense(100, kernel_regularizer=regularizers.l1(0.01)))\n",
    "   \n",
    "   Pros: ‚úÖ Automatic feature selection\n",
    "         ‚úÖ Reduces model size\n",
    "   Cons: ‚ö†Ô∏è May hurt performance\n",
    "         ‚ö†Ô∏è Need to tune Œª\n",
    "\n",
    "2. DROPOUT WITH HIGH RATE (50%+) üíß\n",
    "   ‚Ä¢ Randomly drops 50%+ of neurons during training\n",
    "   ‚Ä¢ At inference: all weights active but scaled\n",
    "   ‚Ä¢ Effectively creates sparse activation patterns\n",
    "   \n",
    "   Code:\n",
    "   model.add(Dropout(0.5))  # or 0.6, 0.7\n",
    "   \n",
    "   Pros: ‚úÖ Strong regularization\n",
    "         ‚úÖ Ensemble effect\n",
    "   Cons: ‚ö†Ô∏è Not truly sparse (weights still exist)\n",
    "         ‚ö†Ô∏è May underfit if too high\n",
    "\n",
    "3. MAGNITUDE PRUNING ‚úÇÔ∏è\n",
    "   ‚Ä¢ Train full model first\n",
    "   ‚Ä¢ Remove smallest weights (set to 0)\n",
    "   ‚Ä¢ Fine-tune remaining weights\n",
    "   ‚Ä¢ Iteratively prune more if needed\n",
    "   \n",
    "   Steps:\n",
    "   a) Train model normally\n",
    "   b) Sort weights by magnitude\n",
    "   c) Set bottom X% to 0 (e.g., 50%)\n",
    "   d) Fine-tune with remaining weights frozen at 0\n",
    "   \n",
    "   Pros: ‚úÖ Controlled sparsity level\n",
    "         ‚úÖ Often maintains accuracy\n",
    "   Cons: ‚ö†Ô∏è Requires extra training step\n",
    "         ‚ö†Ô∏è Manual process\n",
    "\n",
    "üìä COMPARISON:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Method          | Sparsity | Performance | Automatic | Inference Speed\n",
    "----------------|----------|-------------|-----------|----------------\n",
    "L1 Reg          | Medium   | Good        | ‚úÖ Yes     | ‚úÖ Faster\n",
    "Dropout         | Low      | Good        | ‚úÖ Yes     | ‚ùå Same\n",
    "Pruning         | High     | Best        | ‚ùå No      | ‚úÖ Much Faster\n",
    "\n",
    "üéØ WHEN TO USE EACH:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Feature selection needed ‚Üí L1 Regularization\n",
    "- Training regularization ‚Üí Dropout\n",
    "- Deploy to mobile/edge ‚Üí Magnitude Pruning\n",
    "- Want smallest model ‚Üí Combine all three!\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 7: Dropout - Does it slow down training/inference?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 7: Does Dropout slow down training/inference?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ DROPOUT EFFECTS ON SPEED:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üìä TRAINING:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚úÖ YES, Dropout SLOWS DOWN training convergence\n",
    "\n",
    "WHY:\n",
    "- Each iteration uses only subset of neurons (e.g., 80% if dropout=0.2)\n",
    "- Effective network capacity reduced during training\n",
    "- Model needs MORE epochs to converge\n",
    "- Each epoch is slightly faster (fewer neurons active)\n",
    "- But TOTAL training time INCREASES (need ~2x more epochs)\n",
    "\n",
    "TYPICAL IMPACT:\n",
    "- Without Dropout: 20 epochs to converge\n",
    "- With Dropout 20%: 30-40 epochs to converge\n",
    "- Net effect: ~1.5-2x longer training time\n",
    "\n",
    "üìä INFERENCE (PRODUCTION):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚ùå NO, Dropout does NOT slow down inference\n",
    "\n",
    "WHY:\n",
    "- Dropout is TURNED OFF during inference/testing\n",
    "- All neurons are active (no random dropping)\n",
    "- Weights are scaled down by (1 - dropout_rate)\n",
    "- No additional computation compared to no-dropout model\n",
    "- Inference speed IDENTICAL\n",
    "\n",
    "KERAS IMPLEMENTATION:\n",
    "- model.fit() ‚Üí Dropout active (training=True)\n",
    "- model.predict() ‚Üí Dropout OFF (training=False)\n",
    "- Handled automatically!\n",
    "\n",
    "üìä SUMMARY TABLE:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Phase       | Speed Impact | Reason\n",
    "------------|--------------|----------------------------------------\n",
    "Training    | ‚úÖ SLOWER    | Needs more epochs to converge (~2x)\n",
    "Inference   | ‚úÖ SAME      | Dropout turned OFF, all neurons active\n",
    "\n",
    "üéØ PRACTICAL IMPLICATION:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Training: Budget more time/epochs when using Dropout\n",
    "- Production: NO performance penalty, only benefits!\n",
    "- Trade-off: Longer training for better generalization\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISES 8-10: Practical Implementation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù EXERCISES 8-10: Deep Network on Fashion MNIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "These exercises require implementing a deep neural network on Fashion MNIST\n",
    "using all the techniques we've learned. We already completed this in \n",
    "EXPERIMENT 13 (Part 11-12) with the production-grade model!\n",
    "\n",
    "üéØ KEY ACHIEVEMENTS FROM EXPERIMENT 13:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚úÖ Exercise 8: Built deep network with proper initialization\n",
    "‚úÖ Exercise 9: Applied Batch Normalization + Dropout + L2\n",
    "‚úÖ Exercise 10: Used Adam/SGD+Momentum with learning rate scheduling\n",
    "\n",
    "RESULTS ACHIEVED:\n",
    "- Test Accuracy: ~90% (see Experiment 13 output above)\n",
    "- Proper regularization (minimal overfitting)\n",
    "- Fast convergence with callbacks\n",
    "- Production-ready pipeline\n",
    "\n",
    "Refer to the comprehensive Fashion MNIST model trained above! ‚úÖ\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéì CHAPTER 11: FINAL SUMMARY & KEY TAKEAWAYS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ COMPREHENSIVE CHAPTER 11 LEARNINGS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. INITIALIZATION (Critical Foundation)\n",
    "   üèÜ Best Practice: He initialization for ReLU\n",
    "   üìä Impact: 10% ‚Üí 85% accuracy (75 percentage points!)\n",
    "   üí° Never initialize all weights to same value (symmetry!)\n",
    "\n",
    "2. ACTIVATION FUNCTIONS\n",
    "   üèÜ Best Practice: ReLU (default), ELU (better), SELU (very deep)\n",
    "   üìä Impact: Small differences (~1-2%), but critical for deep networks\n",
    "   üí° Avoid sigmoid/tanh in hidden layers (vanishing gradients)\n",
    "\n",
    "3. BATCH NORMALIZATION\n",
    "   üèÜ Best Practice: Use after Dense, before Activation\n",
    "   üìä Impact: +3.2% faster convergence in epoch 1\n",
    "   üí° Enables higher learning rates, acts as regularizer\n",
    "\n",
    "4. TRANSFER LEARNING\n",
    "   üèÜ Best Practice: Works when tasks similar + limited target data\n",
    "   üìä Impact: Variable (can hurt if tasks too different!)\n",
    "   üí° Freeze lower layers first, then fine-tune\n",
    "\n",
    "5. OPTIMIZERS (Game Changer!)\n",
    "   üèÜ Best Practice: SGD + Momentum (0.9) + Nesterov for production\n",
    "   üìä Impact: 86.65% ‚Üí 87.85% (+1.2%)\n",
    "   üí° Adam for prototyping (fast), SGD+Momentum for final models\n",
    "\n",
    "6. LEARNING RATE SCHEDULING\n",
    "   üèÜ Best Practice: ReduceLROnPlateau (adaptive, no tuning)\n",
    "   üìä Impact: +0.25% improvement, helps convergence\n",
    "   üí° Essential for very long training runs\n",
    "\n",
    "7. REGULARIZATION (Production Essential!)\n",
    "   üèÜ Best Practice: Dropout 10-30% (primary), weak L2 (secondary)\n",
    "   üìä Impact: 87.6% ‚Üí 88.8% with Dropout 10% (+1.2%)\n",
    "   üí° Dropout > L2 regularization for neural networks\n",
    "\n",
    "8. COMBINED BEST PRACTICES\n",
    "   üèÜ Best Practice: All techniques together\n",
    "   üìä Impact: Simple model 87.6% ‚Üí Best practices 90.05% (+2.45%)\n",
    "   üí° Production model achieves ~90% on Fashion MNIST!\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üéØ DEFAULT CONFIGURATION (Copy-Paste Ready):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=5),\n",
    "    EarlyStopping(patience=15, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üìö WHAT WE ACCOMPLISHED IN THIS CHAPTER:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚úÖ Understood vanishing/exploding gradients problem\n",
    "‚úÖ Mastered weight initialization strategies\n",
    "‚úÖ Compared activation functions systematically\n",
    "‚úÖ Implemented Batch Normalization properly\n",
    "‚úÖ Explored transfer learning (and its limitations)\n",
    "‚úÖ Tested modern optimizers (SGD, Adam, RMSprop, etc.)\n",
    "‚úÖ Applied learning rate scheduling strategies\n",
    "‚úÖ Mastered regularization (Dropout, L1/L2)\n",
    "‚úÖ Built production-grade models (90%+ accuracy)\n",
    "‚úÖ Solved all chapter exercises\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üöÄ YOU NOW HAVE:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Deep understanding of training deep neural networks\n",
    "- Production-ready code templates\n",
    "- Systematic approach to hyperparameter selection\n",
    "- Debugging strategies for training issues\n",
    "- Best practices for real-world deployment\n",
    "\n",
    "üéì NEXT STEPS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Apply these techniques to your own datasets\n",
    "- Experiment with different architectures (CNN, RNN)\n",
    "- Study Chapter 12 (Custom Models and Training)\n",
    "- Build production models with confidence!\n",
    "\n",
    "üéâ CONGRATULATIONS ON COMPLETING CHAPTER 11! üéâ\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CHAPTER 11 COMPLETE - ALL PARTS FINISHED!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
